{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dougyd92/ML-Foudations/blob/main/Notebooks/6_Classification_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ2pyEWALSpM"
      },
      "source": [
        "# Session 6: Classification Evaluation & Advanced Classification\n",
        "\n",
        "This notebook covers:\n",
        "1. **Confusion Matrix** — understanding prediction errors\n",
        "2. **Classification Metrics** — precision, recall, F1, and beyond\n",
        "3. **Threshold Selection & ROC/AUC** — tuning the decision boundary\n",
        "4. **Precision-Recall Curves** — evaluation for imbalanced settings\n",
        "5. **Handling Imbalanced Datasets** — stratified splits, class weights, SMOTE\n",
        "6. **Multi-class Classification** — OvR, OvO, softmax\n",
        "\n",
        "By the end of this notebook, you'll be able to train a classifier and rigorously evaluate its performance using the right metrics for your problem."
      ],
      "id": "PJ2pyEWALSpM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMXBuswgLSpN"
      },
      "source": [
        "# ============================================================\n",
        "# Setup — Run this cell first!\n",
        "# ============================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer, make_classification, load_iris\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, ConfusionMatrixDisplay,\n",
        "    classification_report, accuracy_score,\n",
        "    precision_score, recall_score, f1_score,\n",
        "    roc_curve, auc, RocCurveDisplay,\n",
        "    precision_recall_curve, average_precision_score, PrecisionRecallDisplay\n",
        ")\n",
        "\n",
        "# For SMOTE (install if needed)\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "    SMOTE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    !pip install imbalanced-learn -q\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "    SMOTE_AVAILABLE = True\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"✅ All imports successful!\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "SMXBuswgLSpN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxsHkpzNLSpN"
      },
      "source": [
        "---\n",
        "# Section 1: The Confusion Matrix\n",
        "\n",
        "Accuracy tells us the fraction of predictions we got right. But when classes are imbalanced, accuracy can be deeply misleading. A model that **always** predicts \"healthy\" on a dataset where 99% of patients are healthy gets 99% accuracy — while missing every single sick patient.\n",
        "\n",
        "The **confusion matrix** breaks down predictions into four categories, giving us a much richer picture of model performance."
      ],
      "id": "IxsHkpzNLSpN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlRkKdC4LSpO"
      },
      "source": [
        "## Building Intuition: A Simple Classifier\n",
        "\n",
        "Let's train a logistic regression model on the **Breast Cancer Wisconsin** dataset and examine its confusion matrix. This dataset has two classes: **malignant** (positive) and **benign** (negative)."
      ],
      "id": "IlRkKdC4LSpO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2kZD8pWLSpO"
      },
      "source": [
        "# Load and prepare the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Note: in this dataset, 1 = benign, 0 = malignant\n",
        "# Let's flip so that 1 = malignant (the class we want to detect)\n",
        "y = 1 - y\n",
        "\n",
        "print(f\"Dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "print(f\"Class distribution: {np.sum(y == 0)} benign, {np.sum(y == 1)} malignant\")\n",
        "print(f\"Malignant rate: {np.mean(y):.1%}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "J2kZD8pWLSpO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIjfrBRSLSpO"
      },
      "source": [
        "# Train/test split and fit logistic regression\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(random_state=42, max_iter=5000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "y_proba = model.predict_proba(X_test_scaled)[:, 1]  # probabilities for positive class\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "iIjfrBRSLSpO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRgg0SNFLSpO"
      },
      "source": [
        "## The Confusion Matrix\n",
        "\n",
        "The confusion matrix is a 2×2 table that cross-tabulates **actual** labels (rows) against **predicted** labels (columns):\n",
        "\n",
        "|  | Predicted Negative | Predicted Positive |\n",
        "|---|---|---|\n",
        "| **Actual Negative** | True Negative (TN) | False Positive (FP) |\n",
        "| **Actual Positive** | False Negative (FN) | True Positive (TP) |\n",
        "\n",
        "- **True Positives (TP):** Correctly identified malignant tumors\n",
        "- **True Negatives (TN):** Correctly identified benign tumors\n",
        "- **False Positives (FP):** Benign tumors incorrectly flagged as malignant (\"false alarms\")\n",
        "- **False Negatives (FN):** Malignant tumors missed by the model (\"misses\") ← *the dangerous ones*"
      ],
      "id": "qRgg0SNFLSpO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_N8KQROLSpO"
      },
      "source": [
        "# Compute and display the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix (raw counts):\")\n",
        "print(cm)\n",
        "print()\n",
        "\n",
        "# Extract the four components\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"True Negatives  (TN): {tn}  — Correctly identified benign\")\n",
        "print(f\"False Positives (FP): {fp}  — Benign incorrectly flagged as malignant\")\n",
        "print(f\"False Negatives (FN): {fn}  — Malignant MISSED by the model\")\n",
        "print(f\"True Positives  (TP): {tp}  — Correctly identified malignant\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "y_N8KQROLSpO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7knV7G1rLSpO"
      },
      "source": [
        "# Visualize with ConfusionMatrixDisplay\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Raw counts\n",
        "ConfusionMatrixDisplay.from_predictions(\n",
        "    y_test, y_pred,\n",
        "    display_labels=[\"Benign\", \"Malignant\"],\n",
        "    cmap=\"Blues\", ax=axes[0]\n",
        ")\n",
        "axes[0].set_title(\"Confusion Matrix (Counts)\")\n",
        "\n",
        "# Normalized by true class (each row sums to 1)\n",
        "ConfusionMatrixDisplay.from_predictions(\n",
        "    y_test, y_pred,\n",
        "    display_labels=[\"Benign\", \"Malignant\"],\n",
        "    normalize='true', cmap=\"Blues\", ax=axes[1],\n",
        "    values_format='.2%'\n",
        ")\n",
        "axes[1].set_title(\"Confusion Matrix (Normalized by Actual)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7knV7G1rLSpO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N26_fz4LSpO"
      },
      "source": [
        "---\n",
        "## ✏️ Exercise 1: Confusion Matrix Interpretation\n",
        "\n",
        "Use the confusion matrix from our breast cancer model above to answer these questions."
      ],
      "id": "-N26_fz4LSpO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASK08GljLSpO"
      },
      "source": [
        "**Task 1:** Calculate the accuracy manually from TP, TN, FP, FN. Verify it matches `accuracy_score`."
      ],
      "id": "ASK08GljLSpO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeIfYvfiLSpO"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "LeIfYvfiLSpO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "PSzT3L4BLSpO"
      },
      "source": [
        "#@title Click to reveal solution.\n",
        "accuracy_manual = (tp + tn) / (tp + tn + fp + fn)\n",
        "print(f\"Manual accuracy:  {accuracy_manual:.3f}\")\n",
        "print(f\"sklearn accuracy: {accuracy_score(y_test, y_pred):.3f}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "PSzT3L4BLSpO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdwSynhgLSpO"
      },
      "source": [
        "**Task 2:** If this model were deployed as a cancer screening tool, which type of error (FP or FN) would be more dangerous? Calculate the **false negative rate** (FN / total actual positives)."
      ],
      "id": "jdwSynhgLSpO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbMEOcrXLSpO"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "DbMEOcrXLSpO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "eUs5FHY-LSpO"
      },
      "source": [
        "#@title Click to reveal solution.\n",
        "fnr = fn / (fn + tp)\n",
        "fpr_val = fp / (fp + tn)\n",
        "print(f\"False Negative Rate: {fnr:.3f} ({fnr:.1%} of malignant tumors missed)\")\n",
        "print(f\"False Positive Rate: {fpr_val:.3f} ({fpr_val:.1%} of benign flagged)\")\n",
        "print()\n",
        "print(\"In cancer screening, FN is far more dangerous — a missed malignant tumor\")\n",
        "print(\"could mean delayed treatment. FP just means an extra biopsy (inconvenient, but safe).\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "eUs5FHY-LSpO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05P-xHItLSpP"
      },
      "source": [
        "**Task 3:** Create a \"dummy\" model that always predicts benign (class 0). Compute its confusion matrix and accuracy. What does this tell you about accuracy as a metric?"
      ],
      "id": "05P-xHItLSpP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6pCSgbJLSpP"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "V6pCSgbJLSpP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "z24nsU-xLSpP"
      },
      "source": [
        "#@title Click to reveal solution.\n",
        "y_pred_dummy = np.zeros_like(y_test)\n",
        "print(f\"Dummy model accuracy: {accuracy_score(y_test, y_pred_dummy):.3f}\")\n",
        "print()\n",
        "cm_dummy = confusion_matrix(y_test, y_pred_dummy)\n",
        "print(\"Confusion Matrix (dummy):\")\n",
        "print(cm_dummy)\n",
        "print()\n",
        "tn_d, fp_d, fn_d, tp_d = cm_dummy.ravel()\n",
        "print(f\"TP={tp_d}, FP={fp_d}, FN={fn_d}, TN={tn_d}\")\n",
        "print()\n",
        "print(\"The dummy model gets decent accuracy by predicting the majority class,\")\n",
        "print(\"but it misses EVERY malignant tumor (TP=0, FN=all positives).\")\n",
        "print(\"This is why accuracy alone is insufficient!\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "z24nsU-xLSpP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7wE8FcRLSpP"
      },
      "source": [
        "---\n",
        "# Section 2: Classification Metrics\n",
        "\n",
        "Now that we understand the confusion matrix, we can derive more informative metrics. Each one answers a different question about model performance."
      ],
      "id": "s7wE8FcRLSpP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCzM9GGtLSpP"
      },
      "source": [
        "## Precision and Recall\n",
        "\n",
        "**Precision** answers: *\"When the model predicts positive, how often is it right?\"*\n",
        "\n",
        "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
        "\n",
        "**Recall** (Sensitivity) answers: *\"Of all actual positives, how many did the model catch?\"*\n",
        "\n",
        "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
        "\n",
        "These metrics capture fundamentally different concerns:\n",
        "- **High precision** → few false alarms (important for spam filtering)\n",
        "- **High recall** → few missed cases (important for disease detection)"
      ],
      "id": "UCzM9GGtLSpP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr9fKVmGLSpP"
      },
      "source": [
        "# Compute precision and recall for our breast cancer model\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"  → When the model says 'malignant', it's correct {precision:.1%} of the time\")\n",
        "print()\n",
        "print(f\"Recall:    {recall:.3f}\")\n",
        "print(f\"  → The model catches {recall:.1%} of all malignant tumors\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "Lr9fKVmGLSpP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWjZzMYTLSpP"
      },
      "source": [
        "## F1 Score\n",
        "\n",
        "The **F1 score** is the harmonic mean of precision and recall, providing a single balanced metric:\n",
        "\n",
        "$$F_1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
        "\n",
        "Why the **harmonic** mean instead of the arithmetic mean? Because the harmonic mean punishes imbalance. If precision = 1.0 and recall = 0.0, the arithmetic mean would be 0.5, but the F1 is 0.0 — correctly reflecting a useless model."
      ],
      "id": "GWjZzMYTLSpP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2quiKhTyLSpP"
      },
      "source": [
        "# F1 score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"F1 Score: {f1:.3f}\")\n",
        "print()\n",
        "\n",
        "# Compare harmonic vs arithmetic mean\n",
        "arith_mean = (precision + recall) / 2\n",
        "print(f\"Arithmetic mean of P and R: {arith_mean:.3f}\")\n",
        "print(f\"Harmonic mean (F1):         {f1:.3f}\")\n",
        "print(\"The harmonic mean is always ≤ the arithmetic mean.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2quiKhTyLSpP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plSNZO22LSpP"
      },
      "source": [
        "## The Classification Report\n",
        "\n",
        "sklearn's `classification_report` gives you everything at a glance: precision, recall, F1, and support (count) for each class, plus macro and weighted averages."
      ],
      "id": "plSNZO22LSpP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK95CSgcLSpP"
      },
      "source": [
        "# The all-in-one classification report\n",
        "print(classification_report(y_test, y_pred, target_names=[\"Benign\", \"Malignant\"]))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "SK95CSgcLSpP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFGTAsoPLSpP"
      },
      "source": [
        "---\n",
        "# Section 3: Threshold Selection & ROC/AUC\n",
        "\n",
        "By default, logistic regression predicts class 1 when the predicted probability exceeds **0.5**. But this threshold is just a default — not a law of nature.\n",
        "\n",
        "By adjusting the threshold, we can trade off between precision and recall to match our application's needs."
      ],
      "id": "zFGTAsoPLSpP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PARAolWLSpP"
      },
      "source": [
        "## How Threshold Affects Predictions\n",
        "\n",
        "Let's see what happens when we change the decision threshold."
      ],
      "id": "9PARAolWLSpP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg8yOQ_4LSpP"
      },
      "source": [
        "# Show how threshold changes predictions\n",
        "thresholds = [0.3, 0.5, 0.7]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "for ax, thresh in zip(axes, thresholds):\n",
        "    y_pred_thresh = (y_proba >= thresh).astype(int)\n",
        "    cm_t = confusion_matrix(y_test, y_pred_thresh)\n",
        "    ConfusionMatrixDisplay(cm_t, display_labels=[\"Benign\", \"Malignant\"]).plot(\n",
        "        ax=ax, cmap=\"Blues\"\n",
        "    )\n",
        "    p = precision_score(y_test, y_pred_thresh, zero_division=0)\n",
        "    r = recall_score(y_test, y_pred_thresh)\n",
        "    ax.set_title(f\"Threshold = {thresh}\\nPrecision={p:.2f}, Recall={r:.2f}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Notice: lower threshold → more positives → higher recall but lower precision\")\n",
        "print(\"        higher threshold → fewer positives → higher precision but lower recall\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "qg8yOQ_4LSpP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mphuwOrLSpP"
      },
      "source": [
        "## The Precision-Recall Trade-off\n",
        "\n",
        "As we sweep the threshold from 0 to 1, precision and recall move in opposite directions. Let's visualize this."
      ],
      "id": "_mphuwOrLSpP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axux-juFLSpP"
      },
      "source": [
        "# Precision and recall as a function of threshold\n",
        "thresholds_range = np.linspace(0.01, 0.99, 200)\n",
        "precisions = []\n",
        "recalls = []\n",
        "\n",
        "for t in thresholds_range:\n",
        "    y_pred_t = (y_proba >= t).astype(int)\n",
        "    precisions.append(precision_score(y_test, y_pred_t, zero_division=0))\n",
        "    recalls.append(recall_score(y_test, y_pred_t, zero_division=0))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(thresholds_range, precisions, label=\"Precision\", linewidth=2)\n",
        "plt.plot(thresholds_range, recalls, label=\"Recall\", linewidth=2)\n",
        "plt.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5, label=\"Default threshold (0.5)\")\n",
        "plt.xlabel(\"Threshold\", fontsize=12)\n",
        "plt.ylabel(\"Score\", fontsize=12)\n",
        "plt.title(\"Precision-Recall Trade-off vs Threshold\", fontsize=14)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "axux-juFLSpP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo_3gr30LSpP"
      },
      "source": [
        "## ROC Curve\n",
        "\n",
        "The **ROC (Receiver Operating Characteristic) curve** plots the True Positive Rate (recall) against the False Positive Rate at every threshold simultaneously.\n",
        "\n",
        "- **Perfect model:** curve passes through the top-left corner (0, 1)\n",
        "- **Random model:** diagonal line from (0, 0) to (1, 1)\n",
        "- **Better models:** curve bows further toward the top-left\n",
        "\n",
        "The **AUC (Area Under the Curve)** summarizes the entire ROC curve as a single number from 0.5 (random) to 1.0 (perfect)."
      ],
      "id": "Zo_3gr30LSpP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-re9Rs6LSpP"
      },
      "source": [
        "# Plot ROC curve\n",
        "fpr, tpr, roc_thresholds = roc_curve(y_test, y_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 7))\n",
        "plt.plot(fpr, tpr, color='steelblue', linewidth=2, label=f'Logistic Regression (AUC = {roc_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Classifier (AUC = 0.5)')\n",
        "plt.fill_between(fpr, tpr, alpha=0.1, color='steelblue')\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
        "plt.ylabel(\"True Positive Rate (Recall)\", fontsize=12)\n",
        "plt.title(\"ROC Curve — Breast Cancer Classification\", fontsize=14)\n",
        "plt.legend(fontsize=11, loc='lower right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"AUC = {roc_auc:.3f}\")\n",
        "print(f\"Interpretation: there is a {roc_auc:.1%} chance that the model\")\n",
        "print(\"ranks a random malignant sample higher than a random benign sample.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "z-re9Rs6LSpP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOpJMTlILSpP"
      },
      "source": [
        "---\n",
        "## ✏️ Exercise 2: Threshold Selection for Medical Screening\n",
        "\n",
        "You're deploying the breast cancer model as a **screening tool**. The clinical team requires that **at least 95% of malignant tumors are detected** (recall ≥ 0.95). False positives are acceptable since they just lead to additional testing."
      ],
      "id": "DOpJMTlILSpP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jzcqvQ0LSpP"
      },
      "source": [
        "**Task 1:** Find the highest threshold that achieves at least 95% recall. *Hint:* use the `roc_thresholds` from the ROC curve computation above, along with the corresponding `tpr` values."
      ],
      "id": "_jzcqvQ0LSpP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91uUW5a-LSpQ"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "91uUW5a-LSpQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "gh9V7GBjLSpQ"
      },
      "source": [
        "#@title Click to reveal solution.\n",
        "# Find the highest threshold where TPR >= 0.95\n",
        "mask = tpr >= 0.95\n",
        "valid_thresholds = roc_thresholds[mask]\n",
        "best_threshold = valid_thresholds.max()  # highest threshold that still meets recall requirement\n",
        "\n",
        "print(f\"Best threshold for ≥95% recall: {best_threshold:.4f}\")\n",
        "\n",
        "# Verify\n",
        "y_pred_clinical = (y_proba >= best_threshold).astype(int)\n",
        "print(f\"Recall at this threshold:    {recall_score(y_test, y_pred_clinical):.3f}\")\n",
        "print(f\"Precision at this threshold: {precision_score(y_test, y_pred_clinical):.3f}\")\n",
        "print(f\"Accuracy at this threshold:  {accuracy_score(y_test, y_pred_clinical):.3f}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "gh9V7GBjLSpQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUzn2YhlLSpQ"
      },
      "source": [
        "**Task 2:** Compare the confusion matrices at threshold=0.5 vs your clinical threshold side by side. How many additional false positives do we accept to meet the recall requirement?"
      ],
      "id": "EUzn2YhlLSpQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmCL8wLFLSpQ"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "pmCL8wLFLSpQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "rT-De7BwLSpQ"
      },
      "source": [
        "#@title Click to reveal solution.\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Default threshold\n",
        "y_pred_default = (y_proba >= 0.5).astype(int)\n",
        "ConfusionMatrixDisplay.from_predictions(\n",
        "    y_test, y_pred_default,\n",
        "    display_labels=[\"Benign\", \"Malignant\"],\n",
        "    cmap=\"Blues\", ax=axes[0]\n",
        ")\n",
        "axes[0].set_title(f\"Default Threshold (0.5)\")\n",
        "\n",
        "# Clinical threshold\n",
        "ConfusionMatrixDisplay.from_predictions(\n",
        "    y_test, y_pred_clinical,\n",
        "    display_labels=[\"Benign\", \"Malignant\"],\n",
        "    cmap=\"Blues\", ax=axes[1]\n",
        ")\n",
        "axes[1].set_title(f\"Clinical Threshold ({best_threshold:.3f})\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "cm_default = confusion_matrix(y_test, y_pred_default)\n",
        "cm_clinical = confusion_matrix(y_test, y_pred_clinical)\n",
        "extra_fp = cm_clinical[0, 1] - cm_default[0, 1]\n",
        "fewer_fn = cm_default[1, 0] - cm_clinical[1, 0]\n",
        "print(f\"Additional false positives accepted: {extra_fp}\")\n",
        "print(f\"Additional malignant tumors caught:  {fewer_fn}\")\n",
        "print(f\"This trade-off is worth it in a medical screening context!\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "rT-De7BwLSpQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD_NF9_cLSpQ"
      },
      "source": [
        "---\n",
        "# Section 4: Precision-Recall Curves\n",
        "\n",
        "ROC curves are widely used, but they can be **overly optimistic** when classes are imbalanced. When the negative class is much larger, even many false positives barely move the False Positive Rate.\n",
        "\n",
        "**Precision-Recall (PR) curves** plot precision vs. recall at every threshold and are more informative for imbalanced problems."
      ],
      "id": "CD_NF9_cLSpQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_PWPikXLSpQ"
      },
      "source": [
        "# Plot ROC and PR curves side by side\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# ROC Curve (manual plot for full control)\n",
        "fpr_plot, tpr_plot, _ = roc_curve(y_test, y_proba)\n",
        "roc_auc_plot = auc(fpr_plot, tpr_plot)\n",
        "axes[0].plot(fpr_plot, tpr_plot, color='steelblue', linewidth=2, label=f'AUC = {roc_auc_plot:.3f}')\n",
        "axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
        "axes[0].set_xlabel(\"False Positive Rate\")\n",
        "axes[0].set_ylabel(\"True Positive Rate\")\n",
        "axes[0].set_title(\"ROC Curve\", fontsize=14)\n",
        "axes[0].legend(fontsize=10)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Precision-Recall Curve (manual plot for full control)\n",
        "prec_plot, rec_plot, _ = precision_recall_curve(y_test, y_proba)\n",
        "ap_plot = average_precision_score(y_test, y_proba)\n",
        "axes[1].plot(rec_plot, prec_plot, color='coral', linewidth=2, label=f'AP = {ap_plot:.3f}')\n",
        "prevalence = np.mean(y_test)\n",
        "axes[1].axhline(y=prevalence, color='gray', linestyle='--', alpha=0.5, label=f'Baseline (prevalence={prevalence:.2f})')\n",
        "axes[1].set_xlabel(\"Recall\")\n",
        "axes[1].set_ylabel(\"Precision\")\n",
        "axes[1].set_title(\"Precision-Recall Curve\", fontsize=14)\n",
        "axes[1].legend(fontsize=10)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "ap = average_precision_score(y_test, y_proba)\n",
        "print(f\"Average Precision (AP): {ap:.3f}\")\n",
        "print(f\"ROC AUC:                {roc_auc:.3f}\")\n",
        "print()\n",
        "print(\"For this relatively balanced dataset, both curves look strong.\")\n",
        "print(\"The difference becomes dramatic with highly imbalanced data (next section).\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5_PWPikXLSpQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqN50OQKLSpQ"
      },
      "source": [
        "---\n",
        "# Section 5: Handling Imbalanced Datasets\n",
        "\n",
        "Most real-world classification problems are imbalanced: fraud detection (0.1% fraud), disease screening (1-5% positive), manufacturing defects (<1%), etc.\n",
        "\n",
        "Let's create a deliberately imbalanced dataset and see how different techniques affect performance."
      ],
      "id": "WqN50OQKLSpQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8U4-HjDKLSpQ"
      },
      "source": [
        "# Create an imbalanced dataset (5% positive class)\n",
        "X_imb, y_imb = make_classification(\n",
        "    n_samples=2000, n_features=20, n_informative=10,\n",
        "    n_redundant=5, weights=[0.95, 0.05],\n",
        "    flip_y=0.02, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Class distribution: {np.sum(y_imb == 0)} negative, {np.sum(y_imb == 1)} positive\")\n",
        "print(f\"Positive rate: {np.mean(y_imb):.1%}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "8U4-HjDKLSpQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OltblUO7LSpR"
      },
      "source": [
        "## The Problem: Naive Approach\n",
        "\n",
        "Let's train a standard logistic regression on this imbalanced data and see what happens."
      ],
      "id": "OltblUO7LSpR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WBeisLTLSpR"
      },
      "source": [
        "# Stratified split — always use stratify for imbalanced data!\n",
        "X_train_imb, X_test_imb, y_train_imb, y_test_imb = train_test_split(\n",
        "    X_imb, y_imb, test_size=0.2, random_state=42, stratify=y_imb\n",
        ")\n",
        "\n",
        "scaler_imb = StandardScaler()\n",
        "X_train_imb_s = scaler_imb.fit_transform(X_train_imb)\n",
        "X_test_imb_s = scaler_imb.transform(X_test_imb)\n",
        "\n",
        "# Train without any imbalance handling\n",
        "model_naive = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model_naive.fit(X_train_imb_s, y_train_imb)\n",
        "y_pred_naive = model_naive.predict(X_test_imb_s)\n",
        "\n",
        "print(\"=== Naive Logistic Regression (no imbalance handling) ===\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test_imb, y_pred_naive):.3f}\")\n",
        "print()\n",
        "print(classification_report(y_test_imb, y_pred_naive, target_names=[\"Negative\", \"Positive\"]))\n",
        "print(\"Notice: high accuracy, but look at recall for the Positive class!\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "1WBeisLTLSpR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFxPKipiLSpR"
      },
      "source": [
        "## Fix 1: Class Weights\n",
        "\n",
        "The simplest approach — tell the model that errors on the minority class cost more."
      ],
      "id": "AFxPKipiLSpR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmjLHIpxLSpR"
      },
      "source": [
        "# Train with balanced class weights\n",
        "model_weighted = LogisticRegression(\n",
        "    class_weight='balanced', random_state=42, max_iter=1000\n",
        ")\n",
        "model_weighted.fit(X_train_imb_s, y_train_imb)\n",
        "y_pred_weighted = model_weighted.predict(X_test_imb_s)\n",
        "\n",
        "print(\"=== Logistic Regression with class_weight='balanced' ===\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test_imb, y_pred_weighted):.3f}\")\n",
        "print()\n",
        "print(classification_report(y_test_imb, y_pred_weighted, target_names=[\"Negative\", \"Positive\"]))\n",
        "print(\"Accuracy dropped, but recall for the Positive class improved significantly!\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "xmjLHIpxLSpR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOwT8x9TLSpR"
      },
      "source": [
        "## Fix 2: SMOTE (Synthetic Minority Oversampling)\n",
        "\n",
        "SMOTE creates **new synthetic samples** by interpolating between existing minority class examples. It picks two nearby minority points and creates a new point somewhere on the line between them."
      ],
      "id": "BOwT8x9TLSpR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpj80LnELSpR"
      },
      "source": [
        "# Apply SMOTE to the training data only\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_imb_s, y_train_imb)\n",
        "\n",
        "print(f\"Before SMOTE: {np.sum(y_train_imb == 0)} negative, {np.sum(y_train_imb == 1)} positive\")\n",
        "print(f\"After SMOTE:  {np.sum(y_train_smote == 0)} negative, {np.sum(y_train_smote == 1)} positive\")\n",
        "print()\n",
        "\n",
        "# Train on SMOTE-resampled data\n",
        "model_smote = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model_smote.fit(X_train_smote, y_train_smote)\n",
        "y_pred_smote = model_smote.predict(X_test_imb_s)\n",
        "\n",
        "print(\"=== Logistic Regression with SMOTE ===\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test_imb, y_pred_smote):.3f}\")\n",
        "print()\n",
        "print(classification_report(y_test_imb, y_pred_smote, target_names=[\"Negative\", \"Positive\"]))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "fpj80LnELSpR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-98BDnuLSpR"
      },
      "source": [
        "## Comparing All Three Approaches\n",
        "\n",
        "Let's compare the ROC and PR curves side by side for all three models."
      ],
      "id": "V-98BDnuLSpR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVVuJ-ZmLSpR"
      },
      "source": [
        "# Get probabilities from all three models\n",
        "y_proba_naive = model_naive.predict_proba(X_test_imb_s)[:, 1]\n",
        "y_proba_weighted = model_weighted.predict_proba(X_test_imb_s)[:, 1]\n",
        "y_proba_smote = model_smote.predict_proba(X_test_imb_s)[:, 1]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# ROC Curves\n",
        "for name, proba, color in [\n",
        "    (\"Naive\", y_proba_naive, \"gray\"),\n",
        "    (\"Class Weights\", y_proba_weighted, \"steelblue\"),\n",
        "    (\"SMOTE\", y_proba_smote, \"coral\"),\n",
        "]:\n",
        "    fpr_i, tpr_i, _ = roc_curve(y_test_imb, proba)\n",
        "    auc_i = auc(fpr_i, tpr_i)\n",
        "    axes[0].plot(fpr_i, tpr_i, label=f\"{name} (AUC={auc_i:.3f})\", linewidth=2, color=color)\n",
        "\n",
        "axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
        "axes[0].set_title(\"ROC Curves\", fontsize=14)\n",
        "axes[0].set_xlabel(\"False Positive Rate\")\n",
        "axes[0].set_ylabel(\"True Positive Rate\")\n",
        "axes[0].legend(fontsize=10)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# PR Curves\n",
        "for name, proba, color in [\n",
        "    (\"Naive\", y_proba_naive, \"gray\"),\n",
        "    (\"Class Weights\", y_proba_weighted, \"steelblue\"),\n",
        "    (\"SMOTE\", y_proba_smote, \"coral\"),\n",
        "]:\n",
        "    prec_i, rec_i, _ = precision_recall_curve(y_test_imb, proba)\n",
        "    ap_i = average_precision_score(y_test_imb, proba)\n",
        "    axes[1].plot(rec_i, prec_i, label=f\"{name} (AP={ap_i:.3f})\", linewidth=2, color=color)\n",
        "\n",
        "prevalence_imb = np.mean(y_test_imb)\n",
        "axes[1].axhline(y=prevalence_imb, color='gray', linestyle='--', alpha=0.5, label=f'Baseline ({prevalence_imb:.2f})')\n",
        "axes[1].set_title(\"Precision-Recall Curves\", fontsize=14)\n",
        "axes[1].set_xlabel(\"Recall\")\n",
        "axes[1].set_ylabel(\"Precision\")\n",
        "axes[1].legend(fontsize=10)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Key observation: The ROC curves look similar for all three models,\")\n",
        "print(\"but the PR curves reveal much bigger differences in the imbalanced setting.\")\n",
        "print(\"This is why PR curves are preferred for imbalanced data!\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "jVVuJ-ZmLSpR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXPgyDd1LSpR"
      },
      "source": [
        "---\n",
        "## ✏️ Exercise 3: Imbalanced Data Strategies\n",
        "\n",
        "Using the imbalanced dataset from above, experiment with stratified cross-validation and compare approaches more rigorously."
      ],
      "id": "BXPgyDd1LSpR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPph_57MLSpR"
      },
      "source": [
        "**Task 1:** Use `StratifiedKFold` with 5 folds to compute the mean cross-validated F1 score for the **naive** model (no class weights). Then do the same for the **class-weighted** model. Which performs better?"
      ],
      "id": "GPph_57MLSpR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTv0lWO1LSpR"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "NTv0lWO1LSpR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "vFR1ZPejLSpR"
      },
      "source": [
        "#@title Click to reveal solution.\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Naive model\n",
        "scores_naive = cross_val_score(\n",
        "    LogisticRegression(random_state=42, max_iter=1000),\n",
        "    scaler_imb.fit_transform(X_imb), y_imb,\n",
        "    cv=skf, scoring='f1'\n",
        ")\n",
        "\n",
        "# Weighted model\n",
        "scores_weighted = cross_val_score(\n",
        "    LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000),\n",
        "    scaler_imb.fit_transform(X_imb), y_imb,\n",
        "    cv=skf, scoring='f1'\n",
        ")\n",
        "\n",
        "print(f\"Naive model    — Mean F1: {scores_naive.mean():.3f} ± {scores_naive.std():.3f}\")\n",
        "print(f\"Weighted model — Mean F1: {scores_weighted.mean():.3f} ± {scores_weighted.std():.3f}\")\n",
        "print()\n",
        "print(\"The weighted model has a substantially higher F1 on the minority class.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "vFR1ZPejLSpR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_MyDK2RLSpR"
      },
      "source": [
        "**Task 2:** Repeat the cross-validation, but this time use `scoring='recall'`. Why might a medical team prefer to optimize for recall over F1?"
      ],
      "id": "9_MyDK2RLSpR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_GebuXzLSpR"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "J_GebuXzLSpR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "9kmkMyTiLSpR"
      },
      "source": [
        "#@title Click to reveal solution.\n",
        "scores_naive_recall = cross_val_score(\n",
        "    LogisticRegression(random_state=42, max_iter=1000),\n",
        "    scaler_imb.fit_transform(X_imb), y_imb,\n",
        "    cv=skf, scoring='recall'\n",
        ")\n",
        "\n",
        "scores_weighted_recall = cross_val_score(\n",
        "    LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000),\n",
        "    scaler_imb.fit_transform(X_imb), y_imb,\n",
        "    cv=skf, scoring='recall'\n",
        ")\n",
        "\n",
        "print(f\"Naive model    — Mean Recall: {scores_naive_recall.mean():.3f} ± {scores_naive_recall.std():.3f}\")\n",
        "print(f\"Weighted model — Mean Recall: {scores_weighted_recall.mean():.3f} ± {scores_weighted_recall.std():.3f}\")\n",
        "print()\n",
        "print(\"A medical team might optimize for recall because missing a positive case\")\n",
        "print(\"(false negative) is far more costly than a false alarm (false positive).\")\n",
        "print(\"F1 balances precision and recall equally, but in medicine, the costs are not equal.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9kmkMyTiLSpR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahb_RexbLSpR"
      },
      "source": [
        "---\n",
        "# Section 6: Multi-class Classification\n",
        "\n",
        "So far we've focused on **binary** classification (two classes). Many real-world problems have **more than two classes**: digit recognition (0–9), species identification, document categorization, etc.\n",
        "\n",
        "There are three main strategies for extending binary classifiers to multi-class problems:\n",
        "1. **One-vs-Rest (OvR):** Train K binary classifiers, one per class\n",
        "2. **One-vs-One (OvO):** Train K(K−1)/2 classifiers, one per pair\n",
        "3. **Softmax (Multinomial):** Natively predict all K classes at once"
      ],
      "id": "Ahb_RexbLSpR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLdC8NaWLSpR"
      },
      "source": [
        "## Demo: Multi-class with the Iris Dataset\n",
        "\n",
        "The classic Iris dataset has 3 classes (setosa, versicolor, virginica) with 4 features. Let's compare OvR and Multinomial approaches."
      ],
      "id": "KLdC8NaWLSpR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08KzlBErLSpS"
      },
      "source": [
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X_iris, y_iris = iris.data, iris.target\n",
        "\n",
        "X_tr_iris, X_te_iris, y_tr_iris, y_te_iris = train_test_split(\n",
        "    X_iris, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
        ")\n",
        "\n",
        "scaler_iris = StandardScaler()\n",
        "X_tr_iris_s = scaler_iris.fit_transform(X_tr_iris)\n",
        "X_te_iris_s = scaler_iris.transform(X_te_iris)\n",
        "\n",
        "# OvR strategy — use OneVsRestClassifier for explicit OvR\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "model_ovr = OneVsRestClassifier(\n",
        "    LogisticRegression(max_iter=1000, random_state=42)\n",
        ")\n",
        "model_ovr.fit(X_tr_iris_s, y_tr_iris)\n",
        "\n",
        "# Multinomial (softmax) strategy — default for LogisticRegression with lbfgs solver\n",
        "model_multi = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42)\n",
        "model_multi.fit(X_tr_iris_s, y_tr_iris)\n",
        "\n",
        "print(\"=== One-vs-Rest (OvR) ===\")\n",
        "print(classification_report(y_te_iris, model_ovr.predict(X_te_iris_s),\n",
        "                            target_names=iris.target_names))\n",
        "\n",
        "print(\"\\n=== Multinomial (Softmax) ===\")\n",
        "print(classification_report(y_te_iris, model_multi.predict(X_te_iris_s),\n",
        "                            target_names=iris.target_names))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "08KzlBErLSpS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTf_gKX7LSpS"
      },
      "source": [
        "## Multi-class Confusion Matrix\n",
        "\n",
        "The confusion matrix generalizes to K×K. The diagonal shows correct predictions; off-diagonal entries reveal which classes get confused with each other."
      ],
      "id": "pTf_gKX7LSpS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsMdxb2NLSpS"
      },
      "source": [
        "# Multi-class confusion matrix\n",
        "y_pred_iris = model_multi.predict(X_te_iris_s)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ConfusionMatrixDisplay.from_predictions(\n",
        "    y_te_iris, y_pred_iris,\n",
        "    display_labels=iris.target_names,\n",
        "    cmap=\"Blues\", ax=ax\n",
        ")\n",
        "ax.set_title(\"Multi-class Confusion Matrix — Iris (Softmax)\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Off-diagonal entries show which species the model confuses.\")\n",
        "print(\"Setosa is perfectly separated; versicolor and virginica are occasionally swapped.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "TsMdxb2NLSpS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAvTFGwhLSpS"
      },
      "source": [
        "## Multi-class Decision Boundaries (2D Visualization)\n",
        "\n",
        "To visualize decision boundaries, we'll use only 2 features (petal length and petal width) so we can plot in 2D."
      ],
      "id": "hAvTFGwhLSpS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AQoSWnZLSpS"
      },
      "source": [
        "# Helper function to plot decision boundaries\n",
        "def plot_decision_boundary_multiclass(X, y, model, scaler, feature_names, class_names, title, ax):\n",
        "    h = 0.02  # step size\n",
        "    X_s = scaler.transform(X)\n",
        "\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    grid_s = scaler.transform(grid)\n",
        "    Z = model.predict(grid_s).reshape(xx.shape)\n",
        "\n",
        "    cmap_bg = ListedColormap(['#FFDDD2', '#DCEDC8', '#B3E5FC'])\n",
        "    cmap_pts = ListedColormap(['#E63946', '#2D6A4F', '#1D3557'])\n",
        "\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_bg)\n",
        "    for i, name in enumerate(class_names):\n",
        "        mask = y == i\n",
        "        ax.scatter(X[mask, 0], X[mask, 1], c=[cmap_pts.colors[i]],\n",
        "                   label=name, edgecolors='k', s=50, alpha=0.8)\n",
        "\n",
        "    ax.set_xlabel(feature_names[0], fontsize=11)\n",
        "    ax.set_ylabel(feature_names[1], fontsize=11)\n",
        "    ax.set_title(title, fontsize=13)\n",
        "    ax.legend(fontsize=9)\n",
        "\n",
        "# Use only petal length and petal width for 2D visualization\n",
        "X_iris_2d = X_iris[:, 2:4]\n",
        "feature_names_2d = [iris.feature_names[2], iris.feature_names[3]]\n",
        "\n",
        "X_tr_2d, X_te_2d, y_tr_2d, y_te_2d = train_test_split(\n",
        "    X_iris_2d, y_iris, test_size=0.3, random_state=42, stratify=y_iris\n",
        ")\n",
        "\n",
        "scaler_2d = StandardScaler()\n",
        "X_tr_2d_s = scaler_2d.fit_transform(X_tr_2d)\n",
        "\n",
        "# Train both strategies on 2D data\n",
        "model_ovr_2d = OneVsRestClassifier(\n",
        "    LogisticRegression(max_iter=1000, random_state=42)\n",
        ")\n",
        "model_ovr_2d.fit(X_tr_2d_s, y_tr_2d)\n",
        "\n",
        "model_multi_2d = LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42)\n",
        "model_multi_2d.fit(X_tr_2d_s, y_tr_2d)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "plot_decision_boundary_multiclass(\n",
        "    X_iris_2d, y_iris, model_ovr_2d, scaler_2d,\n",
        "    feature_names_2d, iris.target_names, \"OvR Decision Boundaries\", axes[0]\n",
        ")\n",
        "plot_decision_boundary_multiclass(\n",
        "    X_iris_2d, y_iris, model_multi_2d, scaler_2d,\n",
        "    feature_names_2d, iris.target_names, \"Multinomial (Softmax) Decision Boundaries\", axes[1]\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Both strategies produce linear decision boundaries.\")\n",
        "print(\"Differences are subtle here; they diverge more with complex, overlapping classes.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "3AQoSWnZLSpS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a8it7seLSpS"
      },
      "source": [
        "---\n",
        "## ✏️ Exercise 4: Multi-class Evaluation\n",
        "\n",
        "Using the Iris dataset with all 4 features (already split and scaled above as `X_tr_iris_s` and `X_te_iris_s`), practice with multi-class metrics."
      ],
      "id": "2a8it7seLSpS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq9oviwkLSpS"
      },
      "source": [
        "**Task 1:** Print the classification report for the multinomial model. Which class has the lowest F1 score? Why might that be?"
      ],
      "id": "rq9oviwkLSpS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ9BwOE6LSpS"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "HZ9BwOE6LSpS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ZzlcMhk6LSpS"
      },
      "source": [
        "#@title Click to reveal solution.\n",
        "y_pred_multi = model_multi.predict(X_te_iris_s)\n",
        "print(classification_report(y_te_iris, y_pred_multi, target_names=iris.target_names))\n",
        "print(\"Setosa typically has a perfect F1 because it's linearly separable.\")\n",
        "print(\"Versicolor and virginica overlap more, so one of them usually has the lowest F1.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ZzlcMhk6LSpS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CUDVX-4LSpS"
      },
      "source": [
        "**Task 2:** Compute the **macro-averaged** and **weighted-averaged** F1 scores manually from the per-class F1 scores. Verify they match what sklearn reports.\n",
        "\n",
        "*Hint:* macro = simple mean; weighted = weighted by support (number of samples per class)."
      ],
      "id": "8CUDVX-4LSpS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxS0fIQXLSpS"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "MxS0fIQXLSpS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "6-GuSMjwLSpS"
      },
      "source": [
        "#@title Click to reveal solution.\n",
        "# Per-class F1 scores\n",
        "f1_per_class = f1_score(y_te_iris, y_pred_multi, average=None)\n",
        "support = np.array([np.sum(y_te_iris == c) for c in range(3)])\n",
        "\n",
        "print(\"Per-class F1 scores:\")\n",
        "for i, name in enumerate(iris.target_names):\n",
        "    print(f\"  {name}: {f1_per_class[i]:.3f} (support={support[i]})\")\n",
        "\n",
        "# Macro average: simple mean\n",
        "macro_manual = f1_per_class.mean()\n",
        "macro_sklearn = f1_score(y_te_iris, y_pred_multi, average='macro')\n",
        "print(f\"\\nMacro F1 (manual):  {macro_manual:.3f}\")\n",
        "print(f\"Macro F1 (sklearn):  {macro_sklearn:.3f}\")\n",
        "\n",
        "# Weighted average: weighted by support\n",
        "weighted_manual = np.average(f1_per_class, weights=support)\n",
        "weighted_sklearn = f1_score(y_te_iris, y_pred_multi, average='weighted')\n",
        "print(f\"\\nWeighted F1 (manual):  {weighted_manual:.3f}\")\n",
        "print(f\"Weighted F1 (sklearn):  {weighted_sklearn:.3f}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6-GuSMjwLSpS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XVHlX4RLSpS"
      },
      "source": [
        "**Task 3:** Plot the multi-class confusion matrix for the OvR model. Compare it visually to the multinomial model's confusion matrix above. Are the same classes confused?"
      ],
      "id": "9XVHlX4RLSpS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvRqo9mkLSpS"
      },
      "source": [
        "# Write your code here"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "XvRqo9mkLSpS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "C3JsldmNLSpS"
      },
      "source": [
        "#@title Click to reveal solution.\n",
        "y_pred_ovr = model_ovr.predict(X_te_iris_s)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(\n",
        "    y_te_iris, y_pred_ovr,\n",
        "    display_labels=iris.target_names,\n",
        "    cmap=\"Blues\", ax=axes[0]\n",
        ")\n",
        "axes[0].set_title(\"OvR Confusion Matrix\", fontsize=14)\n",
        "\n",
        "ConfusionMatrixDisplay.from_predictions(\n",
        "    y_te_iris, y_pred_multi,\n",
        "    display_labels=iris.target_names,\n",
        "    cmap=\"Blues\", ax=axes[1]\n",
        ")\n",
        "axes[1].set_title(\"Multinomial (Softmax) Confusion Matrix\", fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Both models tend to confuse versicolor ↔ virginica.\")\n",
        "print(\"Setosa is linearly separable and almost always perfectly classified.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "C3JsldmNLSpS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6VZxnaXLSpS"
      },
      "source": [
        "---\n",
        "# Summary\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "| Concept | What It Answers | When to Use |\n",
        "|---|---|---|\n",
        "| **Confusion Matrix** | What types of errors is the model making? | Always — it's the foundation |\n",
        "| **Precision** | When model says positive, is it right? | False positives are costly (spam) |\n",
        "| **Recall** | Did the model find all positives? | False negatives are costly (medical) |\n",
        "| **F1 Score** | Balance of precision and recall | Need a single balanced metric |\n",
        "| **ROC / AUC** | Overall discriminative ability | Comparing models, balanced data |\n",
        "| **PR Curve / AP** | Performance on the positive class | Imbalanced datasets |\n",
        "| **Class Weights** | Force model to attend to minority class | Simplest imbalance fix |\n",
        "| **SMOTE** | Generate synthetic minority samples | When class weights aren't enough |\n",
        "| **Stratified Splits** | Preserve class proportions | Always with imbalanced data |\n",
        "| **OvR / OvO** | Extend binary classifiers to multi-class | Strategy depends on model |\n",
        "| **Softmax** | Native multi-class probabilities | Logistic regression, neural networks |\n",
        "\n",
        "## What's Next\n",
        "\n",
        "**Session 7:** Decision Trees & Ensemble Methods — a fundamentally different approach to both classification and regression, followed by random forests and boosting."
      ],
      "id": "t6VZxnaXLSpS"
    }
  ]
}