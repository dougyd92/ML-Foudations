{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 8: Model Selection, Pipelines & Interpretability\n",
    "\n",
    "In this notebook we'll work through the practical tools that turn individual ML algorithms into a **reliable, reproducible workflow**:\n",
    "\n",
    "1. **Cross-Validation Strategies** — Stratified K-Fold\n",
    "2. **Hyperparameter Tuning** — GridSearchCV & RandomizedSearchCV\n",
    "3. **Sklearn Pipelines** — preventing data leakage\n",
    "4. **Feature Selection** — SelectKBest & RFECV\n",
    "5. **Support Vector Machines** — a new classifier with tunable hyperparameters\n",
    "6. **Model Interpretability** — SHAP values\n",
    "\n",
    "We'll use the **Breast Cancer Wisconsin** dataset throughout so we can focus on the tools rather than data wrangling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Setup: Imports\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data & preprocessing\n",
    "from sklearn.datasets import load_breast_cancer, make_classification\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, KFold, StratifiedKFold,\n",
    "    cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Pipelines & feature selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, RFECV\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Interpretability\n",
    "!pip install shap -q\n",
    "import shap\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All imports loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Load and explore the dataset\n",
    "# ============================================================\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='target')  # 0 = malignant, 1 = benign\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(y.value_counts().rename({0: 'malignant', 1: 'benign'}))\n",
    "print(f\"\\nClass balance: {y.mean():.1%} benign\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold out a final test set — we won't touch this until the end\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set:     {X_test.shape[0]} samples\")\n",
    "print(f\"Train class balance: {y_train.mean():.1%} benign\")\n",
    "print(f\"Test class balance:  {y_test.mean():.1%} benign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Cross-Validation Strategies\n",
    "\n",
    "We introduced cross-validation in Session 4. Now we'll look under the hood and see **why the choice of CV strategy matters** — especially for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold vs. Stratified K-Fold\n",
    "\n",
    "Regular K-Fold splits the data into K chunks without regard for the target variable. If the classes are imbalanced, some folds may end up with very different class proportions than the full dataset — leading to unreliable score estimates.\n",
    "\n",
    "**Stratified K-Fold** preserves the class distribution in every fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Compare K-Fold vs Stratified K-Fold: class proportions\n",
    "# ============================================================\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Regular K-Fold — % benign in each fold's TEST set:\")\n",
    "for i, (train_idx, test_idx) in enumerate(kf.split(X_train)):\n",
    "    pct = y_train.iloc[test_idx].mean()\n",
    "    print(f\"  Fold {i+1}: {pct:.1%}\")\n",
    "\n",
    "print(f\"\\nOverall training set: {y_train.mean():.1%}\")\n",
    "\n",
    "print(\"\\nStratified K-Fold — % benign in each fold's TEST set:\")\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X_train, y_train)):\n",
    "    pct = y_train.iloc[test_idx].mean()\n",
    "    print(f\"  Fold {i+1}: {pct:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Impact on model scores: K-Fold vs Stratified K-Fold\n",
    "# ============================================================\n",
    "\n",
    "model = LogisticRegression(max_iter=5000, random_state=42)\n",
    "\n",
    "# Scale for logistic regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "scores_kf = cross_val_score(model, X_train_scaled, y_train, cv=kf, scoring='accuracy')\n",
    "scores_skf = cross_val_score(model, X_train_scaled, y_train, cv=skf, scoring='accuracy')\n",
    "\n",
    "print(\"Logistic Regression accuracy scores per fold:\")\n",
    "print(f\"  K-Fold:           {scores_kf}\")\n",
    "print(f\"    Mean: {scores_kf.mean():.4f}  Std: {scores_kf.std():.4f}\")\n",
    "print(f\"  Stratified K-Fold: {scores_skf}\")\n",
    "print(f\"    Mean: {scores_skf.mean():.4f}  Std: {scores_skf.std():.4f}\")\n",
    "print(\"\\nNotice stratified folds tend to have lower variance across folds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway:** For classification, always use `StratifiedKFold`. The good news — `cross_val_score` and `GridSearchCV` default to stratified splitting when you pass a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Cross-Validation\n",
    "\n",
    "1. Create a `StratifiedKFold` with **10 folds** (shuffle=True, random_state=0).\n",
    "2. Run `cross_val_score` using a `DecisionTreeClassifier(random_state=42)` on the **unscaled** `X_train` with `scoring='f1'`.\n",
    "3. Print the mean and standard deviation of the F1 scores.\n",
    "4. Now repeat with `cv=5`. Does using more folds reduce variance in the scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Click to reveal solution\n",
    "\n",
    "# 1. Create StratifiedKFold with 10 folds\n",
    "skf_10 = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "# 2. Cross-validate a decision tree with F1 scoring\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "scores_10 = cross_val_score(dt, X_train, y_train, cv=skf_10, scoring='f1')\n",
    "\n",
    "# 3. Print mean and std\n",
    "print(\"10-Fold Stratified CV (F1):\")\n",
    "print(f\"  Scores: {scores_10.round(4)}\")\n",
    "print(f\"  Mean:   {scores_10.mean():.4f}\")\n",
    "print(f\"  Std:    {scores_10.std():.4f}\")\n",
    "\n",
    "# 4. Repeat with 5 folds\n",
    "skf_5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "scores_5 = cross_val_score(dt, X_train, y_train, cv=skf_5, scoring='f1')\n",
    "\n",
    "print(\"\\n5-Fold Stratified CV (F1):\")\n",
    "print(f\"  Scores: {scores_5.round(4)}\")\n",
    "print(f\"  Mean:   {scores_5.mean():.4f}\")\n",
    "print(f\"  Std:    {scores_5.std():.4f}\")\n",
    "\n",
    "print(f\"\\nMore folds often means lower variance ({scores_10.std():.4f} vs {scores_5.std():.4f}),\")\n",
    "print(\"because each fold has a larger training set, but also means more computation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Hyperparameter Tuning\n",
    "\n",
    "Model **parameters** (weights, coefficients) are learned during training. **Hyperparameters** (max_depth, C, n_estimators) are set *before* training and control the model's capacity.\n",
    "\n",
    "We can't optimize hyperparameters with gradient descent — we need to **search** for good values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV: Exhaustive Search\n",
    "\n",
    "Grid search tries **every combination** of the hyperparameter values you specify, evaluating each with cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GridSearchCV on a Decision Tree\n",
    "# ============================================================\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "total_combos = 6 * 3 * 3\n",
    "print(f\"Total combinations to evaluate: {total_combos}\")\n",
    "print(f\"With 5-fold CV, that's {total_combos * 5} model fits.\\n\")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,           # use all CPU cores\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV F1 score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Inspect Grid Search results\n",
    "# ============================================================\n",
    "\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Top 10 configurations\n",
    "cols = ['param_max_depth', 'param_min_samples_split', 'param_min_samples_leaf',\n",
    "        'mean_test_score', 'std_test_score', 'mean_train_score', 'rank_test_score']\n",
    "\n",
    "print(\"Top 10 configurations by CV F1 score:\")\n",
    "results[cols].sort_values('rank_test_score').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV: Sampling the Search Space\n",
    "\n",
    "When the search space is large — especially with continuous hyperparameters — trying every combination is impractical. `RandomizedSearchCV` randomly samples a fixed number of combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RandomizedSearchCV on a Random Forest\n",
    "# ============================================================\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# With distributions instead of fixed lists, the space is effectively infinite\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': [3, 5, 7, 10, 15, None],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 10),\n",
    "    'max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_distributions,\n",
    "    n_iter=50,          # try 50 random combinations\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Best CV F1 score: {random_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Compare: Grid Search timing vs Random Search timing\n",
    "# ============================================================\n",
    "import time\n",
    "\n",
    "# Grid search over a moderately-sized space\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "total_grid = 3 * 4 * 3\n",
    "\n",
    "start = time.time()\n",
    "gs = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42), param_grid_rf,\n",
    "    cv=5, scoring='f1', n_jobs=-1\n",
    ")\n",
    "gs.fit(X_train, y_train)\n",
    "grid_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "rs = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=42), param_distributions,\n",
    "    n_iter=total_grid, cv=5, scoring='f1', n_jobs=-1, random_state=42\n",
    ")\n",
    "rs.fit(X_train, y_train)\n",
    "rand_time = time.time() - start\n",
    "\n",
    "print(f\"Grid Search:   {total_grid} combos → best F1 = {gs.best_score_:.4f} ({grid_time:.1f}s)\")\n",
    "print(f\"Random Search: {total_grid} combos → best F1 = {rs.best_score_:.4f} ({rand_time:.1f}s)\")\n",
    "print(\"\\nSame budget, but random search covers a much larger space.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Sklearn Pipelines\n",
    "\n",
    "### The Data Leakage Problem\n",
    "\n",
    "A subtle but critical mistake: if you scale (or impute, or encode) **before** splitting, statistics from the test set leak into your preprocessing. The model sees information it shouldn't, and your evaluation is too optimistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Data leakage demo: WRONG vs RIGHT\n",
    "# ============================================================\n",
    "\n",
    "model = LogisticRegression(max_iter=5000, random_state=42)\n",
    "\n",
    "# --- WRONG: scale everything first, then split ---\n",
    "scaler_wrong = StandardScaler()\n",
    "X_all_scaled = scaler_wrong.fit_transform(X)  # fit on ALL data including test!\n",
    "X_tr_wrong, X_te_wrong, y_tr, y_te = train_test_split(\n",
    "    X_all_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "model.fit(X_tr_wrong, y_tr)\n",
    "acc_wrong = accuracy_score(y_te, model.predict(X_te_wrong))\n",
    "\n",
    "# --- RIGHT: split first, then scale on train only ---\n",
    "X_tr_right, X_te_right, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "scaler_right = StandardScaler()\n",
    "X_tr_right_scaled = scaler_right.fit_transform(X_tr_right)   # fit on train\n",
    "X_te_right_scaled = scaler_right.transform(X_te_right)       # transform test\n",
    "model.fit(X_tr_right_scaled, y_tr)\n",
    "acc_right = accuracy_score(y_te, model.predict(X_te_right_scaled))\n",
    "\n",
    "print(f\"Accuracy with leakage:    {acc_wrong:.4f}\")\n",
    "print(f\"Accuracy without leakage: {acc_right:.4f}\")\n",
    "print(\"\\nThe difference may be small here, but on smaller or noisier datasets\")\n",
    "print(\"it can be substantial — and it ALWAYS gives you a false sense of security.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines: The Clean Solution\n",
    "\n",
    "A `Pipeline` chains preprocessing and modeling into a single object. When you call `.fit()`, each step fits on the training data and transforms it before passing to the next step. When you call `.predict()`, it only transforms (no re-fitting).\n",
    "\n",
    "This makes leakage **impossible by construction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Building a Pipeline\n",
    "# ============================================================\n",
    "\n",
    "# Simple pipeline: scale → classify\n",
    "pipe_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(max_iter=5000, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit and predict — scaler fits on train only, automatically\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "y_pred = pipe_lr.predict(X_test)\n",
    "\n",
    "print(f\"Pipeline accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"\\nPipeline steps: {pipe_lr.named_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cross-validation with a Pipeline — leakage-free by design\n",
    "# ============================================================\n",
    "\n",
    "# The pipeline is re-fit from scratch in each fold\n",
    "scores = cross_val_score(\n",
    "    pipe_lr, X_train, y_train,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1'\n",
    ")\n",
    "\n",
    "print(f\"Pipeline + Stratified 5-Fold CV\")\n",
    "print(f\"  F1 scores: {scores.round(4)}\")\n",
    "print(f\"  Mean: {scores.mean():.4f} ± {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines + Grid Search\n",
    "\n",
    "The real power: you can tune hyperparameters of **any step** in the pipeline. Use the naming convention `stepname__parameter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GridSearchCV with a Pipeline\n",
    "# ============================================================\n",
    "\n",
    "pipe_dt = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Note the double-underscore notation: step_name__param_name\n",
    "param_grid_pipe = {\n",
    "    'model__max_depth': [2, 3, 5, 7, None],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_pipe = GridSearchCV(\n",
    "    pipe_dt, param_grid_pipe,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1', n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_pipe.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best params: {grid_pipe.best_params_}\")\n",
    "print(f\"Best CV F1:  {grid_pipe.best_score_:.4f}\")\n",
    "print(f\"\\nTest set F1: {grid_pipe.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build and Tune a Pipeline\n",
    "\n",
    "1. Create a pipeline with two steps: `StandardScaler` (named `'scaler'`) and `RandomForestClassifier(random_state=42)` (named `'model'`).\n",
    "2. Define a parameter grid that searches over:\n",
    "   - `model__n_estimators`: [50, 100, 200]\n",
    "   - `model__max_depth`: [3, 5, 10, None]\n",
    "3. Run `GridSearchCV` with `scoring='f1'` and 5-fold stratified CV.\n",
    "4. Print the best parameters and the test set accuracy of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Click to reveal solution\n",
    "\n",
    "# 1. Create the pipeline\n",
    "pipe_rf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# 2. Define parameter grid\n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__max_depth': [3, 5, 10, None]\n",
    "}\n",
    "\n",
    "# 3. Run GridSearchCV\n",
    "grid_rf = GridSearchCV(\n",
    "    pipe_rf, param_grid_rf,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1', n_jobs=-1\n",
    ")\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "# 4. Print results\n",
    "print(f\"Best parameters: {grid_rf.best_params_}\")\n",
    "print(f\"Best CV F1: {grid_rf.best_score_:.4f}\")\n",
    "\n",
    "y_pred_rf = grid_rf.predict(X_test)\n",
    "print(f\"Test set accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"\\nTest set classification report:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['malignant', 'benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Feature Selection\n",
    "\n",
    "More features doesn't always mean a better model. Irrelevant features add noise, correlated features cause instability, and fewer features make models simpler, faster, and easier to interpret.\n",
    "\n",
    "We'll look at two practical approaches: **filter methods** and **wrapper methods**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Methods: SelectKBest\n",
    "\n",
    "Score each feature independently using a statistical test (e.g., ANOVA F-test), then keep the top K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SelectKBest: rank features by ANOVA F-score\n",
    "# ============================================================\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k='all')  # score all features\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Visualize feature scores\n",
    "feature_scores = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'f_score': selector.scores_,\n",
    "    'p_value': selector.pvalues_\n",
    "}).sort_values('f_score', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bars = ax.barh(\n",
    "    feature_scores['feature'],\n",
    "    feature_scores['f_score'],\n",
    "    color='steelblue'\n",
    ")\n",
    "ax.set_xlabel('ANOVA F-Score')\n",
    "ax.set_title('Feature Importance: ANOVA F-Test Scores')\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 features:\")\n",
    "print(feature_scores.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SelectKBest inside a Pipeline: how many features is enough?\n",
    "# ============================================================\n",
    "\n",
    "results_by_k = []\n",
    "\n",
    "for k in range(1, X_train.shape[1] + 1):\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('selector', SelectKBest(score_func=f_classif, k=k)),\n",
    "        ('model', LogisticRegression(max_iter=5000, random_state=42))\n",
    "    ])\n",
    "    scores = cross_val_score(\n",
    "        pipe, X_train, y_train,\n",
    "        cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring='f1'\n",
    "    )\n",
    "    results_by_k.append({'k': k, 'mean_f1': scores.mean(), 'std_f1': scores.std()})\n",
    "\n",
    "results_df = pd.DataFrame(results_by_k)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(results_df['k'], results_df['mean_f1'], 'o-', color='steelblue', markersize=4)\n",
    "ax.fill_between(\n",
    "    results_df['k'],\n",
    "    results_df['mean_f1'] - results_df['std_f1'],\n",
    "    results_df['mean_f1'] + results_df['std_f1'],\n",
    "    alpha=0.2, color='steelblue'\n",
    ")\n",
    "best_k = results_df.loc[results_df['mean_f1'].idxmax(), 'k']\n",
    "ax.axvline(best_k, color='coral', linestyle='--', label=f'Best k={int(best_k)}')\n",
    "ax.set_xlabel('Number of Features (k)')\n",
    "ax.set_ylabel('Mean CV F1 Score')\n",
    "ax.set_title('SelectKBest: F1 Score vs. Number of Features')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best k = {int(best_k)} with mean F1 = {results_df['mean_f1'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper Methods: Recursive Feature Elimination (RFECV)\n",
    "\n",
    "Instead of scoring features independently, RFECV uses a model to iteratively remove the weakest feature and evaluates performance at each step using cross-validation. It automatically selects the optimal number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RFECV with a Random Forest\n",
    "# ============================================================\n",
    "\n",
    "# RFECV needs a model with feature_importances_ or coef_\n",
    "rf_for_rfe = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=rf_for_rfe,\n",
    "    step=1,                # remove 1 feature at a time\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "print(f\"\\nSelected features:\")\n",
    "selected = X_train.columns[rfecv.support_].tolist()\n",
    "for f in selected:\n",
    "    print(f\"  • {f}\")\n",
    "\n",
    "print(f\"\\nDropped features:\")\n",
    "dropped = X_train.columns[~rfecv.support_].tolist()\n",
    "for f in dropped:\n",
    "    print(f\"  ✗ {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RFECV: Performance vs number of features\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "n_features_range = range(1, len(rfecv.cv_results_['mean_test_score']) + 1)\n",
    "means = rfecv.cv_results_['mean_test_score']\n",
    "stds = rfecv.cv_results_['std_test_score']\n",
    "\n",
    "ax.plot(n_features_range, means, 'o-', color='steelblue', markersize=4)\n",
    "ax.fill_between(n_features_range, means - stds, means + stds, alpha=0.2, color='steelblue')\n",
    "ax.axvline(rfecv.n_features_, color='coral', linestyle='--',\n",
    "           label=f'Optimal: {rfecv.n_features_} features')\n",
    "ax.set_xlabel('Number of Features')\n",
    "ax.set_ylabel('Mean CV F1 Score')\n",
    "ax.set_title('RFECV: Recursive Feature Elimination with Cross-Validation')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Feature Selection\n",
    "\n",
    "1. Build a pipeline with: `StandardScaler` → `SelectKBest(k=10)` → `LogisticRegression(max_iter=5000, random_state=42)`.\n",
    "2. Use `GridSearchCV` to search over `selector__k` values of [5, 10, 15, 20, 25] with `scoring='f1'` and 5-fold stratified CV.\n",
    "3. Print the best `k` and the corresponding F1 score.\n",
    "4. Which features were selected at the best `k`? Print their names. *(Hint: after fitting, access the selector step from the best estimator with `grid.best_estimator_.named_steps['selector']`, then use `.get_support()` to get a boolean mask.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Click to reveal solution\n",
    "\n",
    "# 1. Build pipeline\n",
    "pipe_fs = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('selector', SelectKBest(score_func=f_classif)),\n",
    "    ('model', LogisticRegression(max_iter=5000, random_state=42))\n",
    "])\n",
    "\n",
    "# 2. Grid search over k\n",
    "param_grid_k = {'selector__k': [5, 10, 15, 20, 25]}\n",
    "\n",
    "grid_fs = GridSearchCV(\n",
    "    pipe_fs, param_grid_k,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1', n_jobs=-1\n",
    ")\n",
    "grid_fs.fit(X_train, y_train)\n",
    "\n",
    "# 3. Best k and score\n",
    "print(f\"Best k: {grid_fs.best_params_['selector__k']}\")\n",
    "print(f\"Best CV F1: {grid_fs.best_score_:.4f}\")\n",
    "\n",
    "# 4. Which features were selected?\n",
    "best_selector = grid_fs.best_estimator_.named_steps['selector']\n",
    "selected_mask = best_selector.get_support()\n",
    "selected_features = X_train.columns[selected_mask].tolist()\n",
    "\n",
    "print(f\"\\nSelected features ({len(selected_features)}):\")\n",
    "for f in selected_features:\n",
    "    print(f\"  • {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Support Vector Machines\n",
    "\n",
    "SVMs find the decision boundary that **maximizes the margin** between classes. With kernel functions, they can model non-linear boundaries without explicitly transforming features.\n",
    "\n",
    "Key hyperparameters:\n",
    "- **C**: tradeoff between margin width and training errors (large C = narrow margin, fewer errors)\n",
    "- **kernel**: linear, rbf, poly\n",
    "- **gamma** (for rbf/poly): controls how far the influence of a single training point reaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Visualize SVM decision boundaries (2D synthetic data)\n",
    "# ============================================================\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Create non-linearly separable data\n",
    "X_circles, y_circles = make_circles(n_samples=300, noise=0.1, factor=0.4, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n",
    "\n",
    "kernels = ['linear', 'rbf', 'poly']\n",
    "\n",
    "for ax, kernel in zip(axes, kernels):\n",
    "    svm = SVC(kernel=kernel, C=1.0, gamma='scale', random_state=42)\n",
    "    svm.fit(X_circles, y_circles)\n",
    "\n",
    "    # Create mesh for decision boundary\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(X_circles[:, 0].min() - 0.5, X_circles[:, 0].max() + 0.5, 200),\n",
    "        np.linspace(X_circles[:, 1].min() - 0.5, X_circles[:, 1].max() + 0.5, 200)\n",
    "    )\n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    ax.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles,\n",
    "               cmap='coolwarm', edgecolors='k', s=30)\n",
    "    acc = svm.score(X_circles, y_circles)\n",
    "    ax.set_title(f'{kernel} kernel (acc: {acc:.2f})')\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "\n",
    "plt.suptitle('SVM Decision Boundaries: Different Kernels', y=1.02, fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The linear kernel can't separate these concentric circles.\")\n",
    "print(\"RBF and polynomial kernels handle non-linear boundaries naturally.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Effect of C on decision boundary\n",
    "# ============================================================\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_demo, y_demo = make_classification(\n",
    "    n_samples=200, n_features=2, n_informative=2, n_redundant=0,\n",
    "    n_clusters_per_class=1, random_state=42\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n",
    "C_values = [0.01, 1.0, 100.0]\n",
    "\n",
    "for ax, C in zip(axes, C_values):\n",
    "    svm = SVC(kernel='rbf', C=C, gamma='scale', random_state=42)\n",
    "    svm.fit(X_demo, y_demo)\n",
    "\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(X_demo[:, 0].min() - 1, X_demo[:, 0].max() + 1, 200),\n",
    "        np.linspace(X_demo[:, 1].min() - 1, X_demo[:, 1].max() + 1, 200)\n",
    "    )\n",
    "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    ax.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    ax.scatter(X_demo[:, 0], X_demo[:, 1], c=y_demo,\n",
    "               cmap='coolwarm', edgecolors='k', s=30)\n",
    "    # Highlight support vectors\n",
    "    ax.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1],\n",
    "               s=100, facecolors='none', edgecolors='gold', linewidths=1.5)\n",
    "    acc = svm.score(X_demo, y_demo)\n",
    "    ax.set_title(f'C={C} ({len(svm.support_vectors_)} SVs, acc: {acc:.2f})')\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "\n",
    "plt.suptitle('Effect of C on SVM Decision Boundary (RBF kernel)', y=1.02, fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Small C → wide margin, more support vectors, tolerates more errors.\")\n",
    "print(\"Large C → narrow margin, fewer support vectors, tries to classify every point correctly.\")\n",
    "print(\"Gold circles = support vectors (the points that define the boundary).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning an SVM with GridSearchCV + Pipeline\n",
    "\n",
    "SVMs are **sensitive to feature scale**, so we always standardize first. We'll search over C, kernel, and gamma using a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Full SVM pipeline with hyperparameter search\n",
    "# ============================================================\n",
    "\n",
    "pipe_svm = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_svm = {\n",
    "    'model__C': [0.1, 1, 10, 100],\n",
    "    'model__kernel': ['linear', 'rbf'],\n",
    "    'model__gamma': ['scale', 'auto', 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Note: gamma is only used for rbf/poly, but sklearn handles this\n",
    "# gracefully — it's ignored for linear kernel.\n",
    "\n",
    "grid_svm = GridSearchCV(\n",
    "    pipe_svm, param_grid_svm,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "grid_svm.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_svm.best_params_}\")\n",
    "print(f\"Best CV F1:      {grid_svm.best_score_:.4f}\")\n",
    "print(f\"\\nTest set results:\")\n",
    "y_pred_svm = grid_svm.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_svm, target_names=['malignant', 'benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: SVM Tuning with RandomizedSearchCV\n",
    "\n",
    "1. Create a pipeline: `StandardScaler` → `SVC(random_state=42, probability=True)`. *(We set `probability=True` because we'll need it for SHAP later.)*\n",
    "2. Define a parameter distribution for `RandomizedSearchCV`:\n",
    "   - `model__C`: `uniform(0.1, 100)` (continuous uniform from 0.1 to ~100)\n",
    "   - `model__kernel`: `['linear', 'rbf', 'poly']`\n",
    "   - `model__gamma`: `['scale', 'auto']`\n",
    "3. Run `RandomizedSearchCV` with `n_iter=30`, `scoring='f1'`, and 5-fold stratified CV.\n",
    "4. Print the best parameters, best CV F1 score, and the test set classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form"
   },
   "outputs": [],
   "source": [
    "#@title Click to reveal solution\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# 1. Create pipeline with probability=True\n",
    "pipe_svm_ex = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', SVC(random_state=42, probability=True))\n",
    "])\n",
    "\n",
    "# 2. Define parameter distributions\n",
    "param_dist_svm = {\n",
    "    'model__C': uniform(0.1, 100),\n",
    "    'model__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'model__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# 3. Run RandomizedSearchCV\n",
    "random_svm = RandomizedSearchCV(\n",
    "    pipe_svm_ex, param_dist_svm,\n",
    "    n_iter=30,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "random_svm.fit(X_train, y_train)\n",
    "\n",
    "# 4. Print results\n",
    "print(f\"Best parameters: {random_svm.best_params_}\")\n",
    "print(f\"Best CV F1: {random_svm.best_score_:.4f}\")\n",
    "\n",
    "print(f\"\\nTest set classification report:\")\n",
    "y_pred_svm_ex = random_svm.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_svm_ex, target_names=['malignant', 'benign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Model Interpretability with SHAP\n",
    "\n",
    "We've seen model-specific interpretability tools:\n",
    "- Linear/logistic regression → **coefficients**\n",
    "- Trees / random forests → **feature importances**\n",
    "\n",
    "**SHAP** (SHapley Additive exPlanations) is **model-agnostic** — it works with *any* model. It assigns each feature a contribution to each individual prediction, based on Shapley values from game theory.\n",
    "\n",
    "Two views:\n",
    "- **Local:** Why did the model make *this specific* prediction?\n",
    "- **Global:** Which features matter most *across all predictions*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Train a model to interpret\n",
    "# We'll use a Random Forest since it's fast with SHAP's TreeExplainer\n",
    "# ============================================================\n",
    "\n",
    "# Fit a tuned RF on the full training set\n",
    "rf_final = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=10, random_state=42\n",
    ")\n",
    "rf_final.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Random Forest test accuracy: {rf_final.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SHAP: Compute Shapley values\n",
    "# ============================================================\n",
    "\n",
    "# TreeExplainer is optimized for tree-based models\n",
    "explainer = shap.TreeExplainer(rf_final)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "print(f\"  {shap_values.shape[0]} test samples × {shap_values.shape[1]} features\")\n",
    "print(f\"\\nEach sample gets a SHAP value per feature, explaining that feature's\")\n",
    "print(f\"contribution to pushing the prediction above or below the base value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Global view: Summary / Beeswarm plot\n",
    "# ============================================================\n",
    "# Each dot = one feature for one sample\n",
    "# Position on x-axis = SHAP value (impact on prediction)\n",
    "# Color = feature value (red = high, blue = low)\n",
    "\n",
    "shap.plots.beeswarm(shap_values[:, :, 1], max_display=15)  # class 1 = benign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the beeswarm plot:**\n",
    "- Features are ranked by overall importance (top = most important).\n",
    "- Each dot is one test sample. The x-axis shows the SHAP value: positive values push the prediction toward \"benign,\" negative toward \"malignant.\"\n",
    "- The color shows the actual feature value for that sample — red = high, blue = low.\n",
    "- This gives you a *global* view of which features matter and *how* they affect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Global view: Bar plot of mean absolute SHAP values\n",
    "# ============================================================\n",
    "\n",
    "shap.plots.bar(shap_values[:, :, 1], max_display=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Local view: Waterfall plot for a single prediction\n",
    "# ============================================================\n",
    "\n",
    "# Pick a specific test sample\n",
    "sample_idx = 0\n",
    "true_label = 'benign' if y_test.iloc[sample_idx] == 1 else 'malignant'\n",
    "pred_label = 'benign' if rf_final.predict(X_test.iloc[[sample_idx]])[0] == 1 else 'malignant'\n",
    "\n",
    "print(f\"Sample {sample_idx}: true = {true_label}, predicted = {pred_label}\")\n",
    "print(f\"\\nWaterfall plot shows how each feature pushed the prediction\")\n",
    "print(f\"away from the base value (average prediction) toward the final output.\\n\")\n",
    "\n",
    "shap.plots.waterfall(shap_values[sample_idx, :, 1], max_display=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Local view: a misclassified sample (if any)\n",
    "# ============================================================\n",
    "\n",
    "y_pred_final = rf_final.predict(X_test)\n",
    "misclassified = np.where(y_pred_final != y_test.values)[0]\n",
    "\n",
    "if len(misclassified) > 0:\n",
    "    idx = misclassified[0]\n",
    "    true_label = 'benign' if y_test.iloc[idx] == 1 else 'malignant'\n",
    "    pred_label = 'benign' if y_pred_final[idx] == 1 else 'malignant'\n",
    "\n",
    "    print(f\"Misclassified sample {idx}: true = {true_label}, predicted = {pred_label}\")\n",
    "    print(f\"SHAP waterfall shows which features led the model astray:\\n\")\n",
    "    shap.plots.waterfall(shap_values[idx, :, 1], max_display=12)\n",
    "else:\n",
    "    print(\"No misclassified samples in the test set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Compare: SHAP importance vs built-in feature_importances_\n",
    "# ============================================================\n",
    "\n",
    "# Built-in (Gini / impurity-based)\n",
    "builtin_imp = pd.Series(rf_final.feature_importances_, index=X_train.columns)\n",
    "builtin_imp = builtin_imp.sort_values(ascending=False).head(10)\n",
    "\n",
    "# SHAP-based (mean absolute SHAP value)\n",
    "shap_imp = pd.Series(\n",
    "    np.abs(shap_values[:, :, 1].values).mean(axis=0),\n",
    "    index=X_train.columns\n",
    ")\n",
    "shap_imp = shap_imp.sort_values(ascending=False).head(10)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].barh(builtin_imp.index[::-1], builtin_imp.values[::-1], color='steelblue')\n",
    "axes[0].set_title('Built-in Feature Importance (Gini)')\n",
    "axes[0].set_xlabel('Importance')\n",
    "\n",
    "axes[1].barh(shap_imp.index[::-1], shap_imp.values[::-1], color='coral')\n",
    "axes[1].set_title('SHAP Feature Importance')\n",
    "axes[1].set_xlabel('Mean |SHAP value|')\n",
    "\n",
    "plt.suptitle('Feature Importance: Built-in vs SHAP', y=1.02, fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Rankings often agree on the top features but can differ in the middle.\")\n",
    "print(\"SHAP is generally more reliable — it measures actual impact on predictions,\")\n",
    "print(\"while Gini importance can overweight high-cardinality or noisy features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "**What we covered:**\n",
    "\n",
    "| Tool | What it does | Key takeaway |\n",
    "|---|---|---|\n",
    "| **Stratified K-Fold** | Preserves class proportions in CV folds | Default for classification |\n",
    "| **GridSearchCV** | Exhaustive hyperparameter search | Best for small search spaces |\n",
    "| **RandomizedSearchCV** | Random sampling of hyperparameters | Better for large / continuous spaces |\n",
    "| **Pipelines** | Chain preprocessing + model | Prevents data leakage by design |\n",
    "| **SelectKBest** | Filter-based feature selection | Fast, model-independent |\n",
    "| **RFECV** | Wrapper-based feature selection | Uses model performance, picks optimal k |\n",
    "| **SVM** | Maximum-margin classifier with kernels | Powerful for non-linear boundaries |\n",
    "| **SHAP** | Model-agnostic feature explanations | Local (per-prediction) + global (overall) |\n",
    "\n",
    "**The workflow pattern:**\n",
    "\n",
    "1. Build a **pipeline** (preprocessing + model)\n",
    "2. Define a **search space** for hyperparameters\n",
    "3. Use **GridSearchCV / RandomizedSearchCV** with **stratified CV**\n",
    "4. Evaluate on **held-out test set**\n",
    "5. Interpret with **SHAP**"
   ]
  }
 ]
}
