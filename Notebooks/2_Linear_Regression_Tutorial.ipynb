{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dougyd92/ML-Foudations/blob/main/Notebooks/2_Linear_Regression_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1qrWZoEMc9H"
      },
      "source": [
        "\n",
        "# Linear Regression: Interactive Tutorial\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wJ8S00FinuNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RJir_mHViaE_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp1_7v-uMc9I"
      },
      "source": [
        "\n",
        "In this notebook, we'll implement linear regression three different ways:\n",
        "1. **scikit-learn** — the standard, production-ready approach\n",
        "2. **NumPy** — manual least squares to see the math in action\n",
        "3. **PyTorch** — gradient descent, the foundation for deep learning\n",
        "\n",
        "Along the way, you'll build intuition for what \"fitting a line\" really means and why these different approaches all arrive at the same answer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txm2NbhlMc9I"
      },
      "source": [
        "# Section 0: Setup and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5vhkBnGMc9I"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwgfZXfvMc9J"
      },
      "source": [
        "## Our Dataset\n",
        "\n",
        "We'll use the **California Housing** dataset from scikit-learn. It contains information about housing districts in California, and the target is the **median house value** (in $100,000s).\n",
        "\n",
        "This is a real dataset — messy, imperfect, and much more interesting than toy examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD1Lrr6YMc9J"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "print(housing.DESCR[:1500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_nlUyYkMc9J"
      },
      "outputs": [],
      "source": [
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y = pd.Series(housing.target, name='MedHouseVal')\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Number of samples: {X.shape[0]}\")\n",
        "print(f\"Number of features: {X.shape[1]}\")\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRu1-DxCMc9J"
      },
      "outputs": [],
      "source": [
        "X.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaH6oPQcMc9J"
      },
      "outputs": [],
      "source": [
        "y.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG2mgSJ7Mc9K"
      },
      "source": [
        "## Visualizing the Data\n",
        "\n",
        "Let's look at how a couple of individual features relate to the target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYmX24RcMc9K"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
        "\n",
        "axes[0].scatter(X['MedInc'], y, alpha=0.1, s=5)\n",
        "axes[0].set_xlabel('Median Income')\n",
        "axes[0].set_ylabel('Median House Value ($100k)')\n",
        "axes[0].set_title('Income vs House Value')\n",
        "\n",
        "axes[1].scatter(X['AveRooms'], y, alpha=0.1, s=5)\n",
        "axes[1].set_xlabel('Average Rooms')\n",
        "axes[1].set_ylabel('Median House Value ($100k)')\n",
        "axes[1].set_title('Rooms vs House Value')\n",
        "axes[1].set_xlim(0, 15)  # Zoom in — there are outliers\n",
        "\n",
        "axes[2].scatter(X['HouseAge'], y, alpha=0.1, s=5)\n",
        "axes[2].set_xlabel('House Age (years)')\n",
        "axes[2].set_ylabel('Median House Value ($100k)')\n",
        "axes[2].set_title('Age vs House Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tzHE-dmMc9K"
      },
      "source": [
        "Notice how **Median Income** has a pretty clear positive relationship with house value. **Average Rooms** has a weaker trend with lots of spread. **House Age** doesn't show much of a linear pattern at all. This is typical of real data — some features are more useful than others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVxkhCrBMc9K"
      },
      "source": [
        "## Train-Test Split\n",
        "\n",
        "Before we do any modeling, we split the data. The model learns from the **training set** and we evaluate on the **test set** to see how it generalizes to data it has never seen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtO-aL40Mc9K"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set:     {X_test.shape[0]} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFq-rEc0Mc9K"
      },
      "source": [
        "---\n",
        "# Section 1: Linear Regression with scikit-learn\n",
        "\n",
        "This is the standard way you'll do linear regression in practice. scikit-learn handles all the math for you and provides a clean, consistent API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IShzwUfPMc9K"
      },
      "source": [
        "## Simple Linear Regression (One Feature)\n",
        "\n",
        "Let's start with just **Median Income** as our single feature to keep things easy to visualize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEw7fvZ0Mc9K"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBDV0bEIMc9K"
      },
      "outputs": [],
      "source": [
        "# Use only Median Income as the feature\n",
        "X_train_simple = X_train[['MedInc']]\n",
        "X_test_simple = X_test[['MedInc']]\n",
        "\n",
        "# Create and fit the model\n",
        "model_simple = LinearRegression()\n",
        "model_simple.fit(X_train_simple, y_train)\n",
        "\n",
        "# The learned parameters\n",
        "print(f\"Slope (w1):     {model_simple.coef_[0]:.4f}\")\n",
        "print(f\"Intercept (w0): {model_simple.intercept_:.4f}\")\n",
        "print(f\"\\nModel: y = {model_simple.intercept_:.4f} + {model_simple.coef_[0]:.4f} * MedInc\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReHsR4hRMc9L"
      },
      "source": [
        "The slope tells us: for each unit increase in median income (roughly $10,000), the predicted median house value increases by about $41,000. That makes intuitive sense — wealthier neighborhoods have more expensive houses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3v3JGBnMc9L"
      },
      "outputs": [],
      "source": [
        "# Predictions on the test set\n",
        "y_pred_simple = model_simple.predict(X_test_simple)\n",
        "\n",
        "# Evaluate\n",
        "mse_simple = mean_squared_error(y_test, y_pred_simple)\n",
        "r2_simple = r2_score(y_test, y_pred_simple)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse_simple:.4f}\")\n",
        "print(f\"R² Score:           {r2_simple:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6FCSTCtMc9L"
      },
      "source": [
        "**R²** tells us what fraction of the variance in house values is explained by our model. An R² of ~0.47 means median income alone explains about 47% of the variation — not bad for a single feature, but there's clearly more to the story."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfOsZ4lnMc9L"
      },
      "outputs": [],
      "source": [
        "# Visualize the fit\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_test_simple, y_test, alpha=0.15, s=8, label='Actual')\n",
        "\n",
        "# Plot the regression line\n",
        "x_line = np.linspace(X_test_simple.min().values[0], X_test_simple.max().values[0], 100).reshape(-1, 1)\n",
        "y_line = model_simple.predict(x_line)\n",
        "plt.plot(x_line, y_line, color='red', linewidth=2, label='Fitted Line')\n",
        "\n",
        "plt.xlabel('Median Income')\n",
        "plt.ylabel('Median House Value ($100k)')\n",
        "plt.title(f'Simple Linear Regression (R² = {r2_simple:.3f})')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4sMzAWDMc9L"
      },
      "source": [
        "## Multiple Linear Regression (All Features)\n",
        "\n",
        "Now let's use **all 8 features**. The equation becomes:\n",
        "\n",
        "ŷ = w₀ + w₁·MedInc + w₂·HouseAge + w₃·AveRooms + ... + w₈·Longitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hbBU34hMc9L"
      },
      "outputs": [],
      "source": [
        "# Fit on all features\n",
        "model_multi = LinearRegression()\n",
        "model_multi.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "y_pred_multi = model_multi.predict(X_test)\n",
        "y_pred_multi\n"
      ],
      "metadata": {
        "id": "Ha2lHV_AXXbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "mse_multi = mean_squared_error(y_test, y_pred_multi)\n",
        "r2_multi = r2_score(y_test, y_pred_multi)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse_multi:.4f}\")\n",
        "print(f\"R² Score:           {r2_multi:.4f}\")\n",
        "print(f\"\\nImprovement over simple model: R² went from {r2_simple:.3f} to {r2_multi:.3f}\")"
      ],
      "metadata": {
        "id": "SyGC_ZrKXcqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STWnS30NMc9L"
      },
      "outputs": [],
      "source": [
        "# Look at all the coefficients\n",
        "coef_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': model_multi.coef_\n",
        "}).sort_values('Coefficient', key=abs, ascending=False)\n",
        "\n",
        "print(f\"Intercept: {model_multi.intercept_:.4f}\\n\")\n",
        "print(coef_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DB9RMNMRMc9L"
      },
      "outputs": [],
      "source": [
        "# Visualize: predicted vs actual\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(y_test, y_pred_multi, alpha=0.15, s=8)\n",
        "plt.plot([0, 5], [0, 5], 'r--', linewidth=2, label='Perfect predictions')\n",
        "plt.xlabel('Actual House Value ($100k)')\n",
        "plt.ylabel('Predicted House Value ($100k)')\n",
        "plt.title(f'Multiple Linear Regression: Predicted vs Actual (R² = {r2_multi:.3f})')\n",
        "plt.legend()\n",
        "plt.xlim(0, 5.2)\n",
        "plt.ylim(0, 5.2)\n",
        "plt.gca().set_aspect('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP0gnAdZMc9L"
      },
      "source": [
        "If the model were perfect, all points would fall exactly on the red dashed line. Points above the line are underpredictions; points below are overpredictions. Notice the cluster of points capped at 5.0 — that's the dataset's maximum value, and our model can't capture that ceiling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq-qwTI3Mc9L"
      },
      "source": [
        "## EXERCISE 1: Exploring the scikit-learn Model\n",
        "\n",
        "1. Make a prediction: what does the model predict for a district with median income of 5.0, house age of 20, average 5 rooms, average 1 bedroom, population of 1000, average occupancy of 3, latitude 34.0, and longitude -118.0? (Hint: you can pass a 2D array or DataFrame to `model_multi.predict()`)\n",
        "2. Compute the **residuals** (actual - predicted) on the test set. What is the mean residual? What is the largest overprediction?\n",
        "3. Create a bar chart of the feature coefficients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyAp16BzMc9L"
      },
      "outputs": [],
      "source": [
        "# 1. Predict for a specific district\n",
        "\n",
        "# Write your code here\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Click to reveal solution.\n",
        "\n",
        "# Version 1: Raw np array\n",
        "x_new = np.array([[5.0, 20, 5, 1, 1000, 3, 34.0, -118.0]])\n",
        "predict_new = model_multi.predict(x_new)\n",
        "print(f\"Predicted median house value: ${predict_new[0] * 100_000:,.0f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Vp0Yl9TTiqxx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title Click to reveal solution.\n",
        "\n",
        "# Version 2: Dataframe with named features\n",
        "new_district = pd.DataFrame([{\n",
        "    'MedInc': 5.0,\n",
        "    'HouseAge': 20,\n",
        "    'AveRooms': 5,\n",
        "    'AveBedrms': 1,\n",
        "    'Population': 1000,\n",
        "    'AveOccup': 3,\n",
        "    'Latitude': 34.0,\n",
        "    'Longitude': -118.0\n",
        "}])\n",
        "\n",
        "predict_new = model_multi.predict(new_district)\n",
        "print(f\"Predicted median house value: ${predict_new[0] * 100_000:,.0f}\")"
      ],
      "metadata": {
        "id": "povwcQOLi1SE",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ErzssrHMc9L"
      },
      "outputs": [],
      "source": [
        "# 2. Compute residuals, mean residual, and largest overprediction\n",
        "\n",
        "# Write your code here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Click to reveal solution.\n",
        "residuals = y_test.values - y_pred_multi\n",
        "\n",
        "print(f\"Mean residual:         {residuals.mean():.4f}\")\n",
        "print(f\"Largest overprediction: {residuals.min():.4f}\")\n",
        "print(f\"  (actual was {residuals.min():.2f} lower than predicted)\")"
      ],
      "metadata": {
        "id": "HgaDipvPbZvk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BmebvaoMc9L"
      },
      "outputs": [],
      "source": [
        "# 3. Bar chart of feature coefficients\n",
        "\n",
        "# Write your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Click to reveal solution.\n",
        "coef_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': model_multi.coef_\n",
        "}).sort_values('Coefficient')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.barh(coef_df['Feature'], coef_df['Coefficient'])\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Linear Regression Coefficients')\n",
        "plt.axvline(x=0, color='black', linewidth=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "z5b0oWwkjv6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "383wijoCMc9L"
      },
      "source": [
        "---\n",
        "# Section 2: Manual Least Squares with NumPy\n",
        "\n",
        "Now let's peek under the hood. scikit-learn is solving the **Normal Equation**:\n",
        "\n",
        "$$w^* = (A^T A)^{-1} A^T y$$\n",
        "\n",
        "where **A** is the feature matrix with a column of 1s prepended (for the intercept).\n",
        "\n",
        "Let's implement this ourselves with nothing but NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps4bjP7YMc9M"
      },
      "source": [
        "## Step 1: Build the Design Matrix\n",
        "\n",
        "We need to add a column of ones to account for the intercept term. This turns our feature matrix **X** into the design matrix **A**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS-LX-V0Mc9M"
      },
      "outputs": [],
      "source": [
        "# Convert to NumPy arrays\n",
        "X_train_np = X_train.values\n",
        "y_train_np = y_train.values\n",
        "X_test_np = X_test.values\n",
        "y_test_np = y_test.values\n",
        "\n",
        "print(f\"X_train shape: {X_train_np.shape}\")\n",
        "print(f\"y_train shape: {y_train_np.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3zjJwH8Mc9M"
      },
      "outputs": [],
      "source": [
        "# Prepend a column of ones for the intercept\n",
        "A_train = np.c_[np.ones(X_train_np.shape[0]), X_train_np]\n",
        "A_test = np.c_[np.ones(X_test_np.shape[0]), X_test_np]\n",
        "\n",
        "print(f\"Design matrix A_train shape: {A_train.shape}\")\n",
        "print(f\"\\nFirst 3 rows of A_train (notice the leading 1s):\")\n",
        "print(A_train[:3, :4], \"...\")  # Show first 4 columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W67P9Ck-Mc9M"
      },
      "source": [
        "## Step 2: Solve the Normal Equation\n",
        "\n",
        "$$w^* = (A^T A)^{-1} A^T y$$\n",
        "\n",
        "In NumPy, the `@` operator performs matrix multiplication, `.T` transposes, and `np.linalg.inv()` computes the inverse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igv9qRKoMc9M"
      },
      "outputs": [],
      "source": [
        "# The Normal Equation in one line\n",
        "w_star = np.linalg.inv(A_train.T @ A_train) @ A_train.T @ y_train_np\n",
        "\n",
        "print(f\"Weights shape: {w_star.shape}\")\n",
        "print(f\"\\nIntercept (w0): {w_star[0]:.4f}\")\n",
        "print(f\"\\nFeature weights:\")\n",
        "for name, w in zip(X.columns, w_star[1:]):\n",
        "    print(f\"  {name:>12s}: {w:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHYaLkw9Mc9M"
      },
      "source": [
        "## Step 3: Verify — Do We Match scikit-learn?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiVpvc2HMc9M"
      },
      "outputs": [],
      "source": [
        "# Compare our weights to sklearn's\n",
        "print(\"Weight comparison (NumPy vs scikit-learn):\")\n",
        "print(f\"{'':>12s}  {'NumPy':>10s}  {'sklearn':>10s}  {'Match?':>8s}\")\n",
        "print(f\"{'Intercept':>12s}  {w_star[0]:>10.4f}  {model_multi.intercept_:>10.4f}  {'✓' if np.isclose(w_star[0], model_multi.intercept_) else '✗':>8s}\")\n",
        "\n",
        "for name, w_np, w_sk in zip(X.columns, w_star[1:], model_multi.coef_):\n",
        "    match = '✓' if np.isclose(w_np, w_sk) else '✗'\n",
        "    print(f\"{name:>12s}  {w_np:>10.4f}  {w_sk:>10.4f}  {match:>8s}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVK62wP7Mc9N"
      },
      "source": [
        "The weights are identical (within floating-point precision). scikit-learn is doing exactly this math under the hood."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnhoZA_JMc9N"
      },
      "outputs": [],
      "source": [
        "# Make predictions and evaluate\n",
        "y_pred_numpy = A_test @ w_star\n",
        "\n",
        "mse_numpy = np.mean((y_test_np - y_pred_numpy) ** 2)\n",
        "r2_numpy = 1 - np.sum((y_test_np - y_pred_numpy) ** 2) / np.sum((y_test_np - np.mean(y_test_np)) ** 2)\n",
        "\n",
        "print(f\"NumPy   — MSE: {mse_numpy:.4f}, R²: {r2_numpy:.4f}\")\n",
        "print(f\"sklearn — MSE: {mse_multi:.4f}, R²: {r2_multi:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UxxMyv6Mc9N"
      },
      "source": [
        "## A Note on `np.linalg.inv`\n",
        "\n",
        "Computing the full matrix inverse is not the most numerically stable approach. In practice, `np.linalg.lstsq` uses a more robust method (SVD decomposition). Let's see:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXXKfb7VMc9N"
      },
      "outputs": [],
      "source": [
        "# More numerically stable approach\n",
        "w_lstsq, residuals, rank, sv = np.linalg.lstsq(A_train, y_train_np, rcond=None)\n",
        "\n",
        "print(\"Max difference between inv() and lstsq():\")\n",
        "print(f\"  {np.max(np.abs(w_star - w_lstsq)):.2e}\")\n",
        "print(\"\\nEffectively identical for this dataset.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx3T2jr2Mc9N"
      },
      "source": [
        "## EXERCISE 2: Simple Regression by Hand\n",
        "\n",
        "For simple linear regression (one feature), the closed-form solution simplifies to:\n",
        "\n",
        "- slope = covariance(x, y) / variance(x)\n",
        "- intercept = mean(y) - slope * mean(x)\n",
        "\n",
        "1. Using only `np.mean`, `np.var`, and `np.cov`, compute the slope and intercept for predicting house value from MedInc alone.\n",
        "2. Compare your result to `model_simple.coef_` and `model_simple.intercept_` from Section 1.\n",
        "\n",
        "Hint: `np.cov(a, b)` returns a 2×2 covariance matrix; the off-diagonal element `[0, 1]` is the covariance between a and b. Use `ddof=0` in `np.var` to match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6crqOOa5Mc9N"
      },
      "outputs": [],
      "source": [
        "# Compute slope and intercept for simple regression using the formulas\n",
        "x_simple = X_train['MedInc'].values\n",
        "y_simple = y_train.values\n",
        "\n",
        "# Your code here:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Click to reveal solution.\n",
        "cov_matrix = np.cov(x_simple, y_simple, ddof=0)\n",
        "slope = cov_matrix[0, 1] / np.var(x_simple, ddof=0)\n",
        "intercept = np.mean(y_simple) - slope * np.mean(x_simple)\n",
        "\n",
        "print(f\"Your slope:     {slope:.4f}  |  sklearn: {model_simple.coef_[0]:.4f}\")\n",
        "print(f\"Your intercept: {intercept:.4f}  |  sklearn: {model_simple.intercept_:.4f}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4aFMC9g1j6A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZXmteI6Mc9N"
      },
      "outputs": [],
      "source": [
        "# Compare to sklearn\n",
        "# print(f\"Your slope:     {slope:.4f}  |  sklearn: {model_simple.coef_[0]:.4f}\")\n",
        "# print(f\"Your intercept: {intercept:.4f}  |  sklearn: {model_simple.intercept_:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9rYDwm0Mc9N"
      },
      "source": [
        "---\n",
        "# Section 3: Gradient Descent with PyTorch\n",
        "\n",
        "The Normal Equation gives us an exact answer, but it requires computing a matrix inverse — which becomes expensive with many features or samples. **Gradient descent** is an iterative alternative:\n",
        "\n",
        "1. Start with random weights\n",
        "2. Compute the loss (MSE)\n",
        "3. Compute the gradient — which direction reduces the loss?\n",
        "4. Take a small step in that direction\n",
        "5. Repeat\n",
        "\n",
        "This is how neural networks learn. We're introducing it here with linear regression so the concept is familiar when we get to deep learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UjPxuGmNMc9N"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl0TsmILMc9N"
      },
      "source": [
        "## Feature Scaling: Why It Matters for Gradient Descent\n",
        "\n",
        "The Normal Equation doesn't care about the scale of features. Gradient descent does — a lot. If one feature ranges from 0–50 and another from 0–100,000, the gradients will be wildly different sizes and training becomes unstable.\n",
        "\n",
        "We'll standardize the features to have mean 0 and standard deviation 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfXo4zRKMc9N"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\"Before scaling — feature means:\", X_train.mean().values.round(2))\n",
        "print(\"After scaling  — feature means:\", X_train_scaled.mean(axis=0).round(4))\n",
        "print(\"After scaling  — feature stds: \", X_train_scaled.std(axis=0).round(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiNuscchMc9N"
      },
      "source": [
        "## Setting Up the Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_gd(learning_rate, n_epochs, X_t, y_t, n_features):\n",
        "    \"\"\"Run gradient descent and return loss history.\"\"\"\n",
        "    torch.manual_seed(42)\n",
        "    w = torch.randn(n_features, 1, requires_grad=True)\n",
        "    b = torch.randn(1, requires_grad=True)\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        # Forward pass: compute predictions\n",
        "        y_pred = X_t @ w + b\n",
        "\n",
        "        # Compute MSE loss\n",
        "        loss = torch.mean((y_pred - y_t) ** 2)\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights (gradient descent step)\n",
        "        with torch.no_grad():\n",
        "            w -= learning_rate * w.grad\n",
        "            b -= learning_rate * b.grad\n",
        "\n",
        "        # Zero out gradients for next iteration\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "\n",
        "        # Print progress every 200 epochs\n",
        "        if (epoch + 1) % 200 == 0:\n",
        "            print(f\"Epoch {epoch+1:>4d}/{n_epochs} — Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return w, b, loss_history"
      ],
      "metadata": {
        "id": "L7SV7aBwkvk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3F0qOfQjMc9N"
      },
      "outputs": [],
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_t = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_t = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "X_test_t = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_t = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "n_features = X_t.shape[1]\n",
        "print(f\"Training tensors: X={X_t.shape}, y={y_t.shape}\")\n",
        "print(f\"Number of features: {n_features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDUQE1KPMc9O"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "n_epochs = 1000\n",
        "\n",
        "w, b, loss_history = run_gd(learning_rate, n_epochs, X_t, y_t, n_features)\n",
        "\n",
        "print(f\"\\nFinal loss: {loss_history[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDZQVRGIMc9O"
      },
      "source": [
        "## Visualizing the Training Process\n",
        "\n",
        "One of the most important things to check: is the loss actually going down?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIRYTg3gMc9O"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_history, linewidth=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('Training Loss Over Time')\n",
        "plt.yscale('log')  # Log scale to see early and late progress\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykyIAuPjMc9O"
      },
      "source": [
        "The loss drops rapidly at first, then levels off as the weights approach the optimal values. This curve is typical of gradient descent — fast early progress followed by gradual refinement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIcEq5vzMc9O"
      },
      "source": [
        "## Evaluating the Gradient Descent Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gs1QYZbMc9O"
      },
      "outputs": [],
      "source": [
        "# Predict on test set\n",
        "with torch.no_grad():\n",
        "    y_pred_gd = (X_test_t @ w + b).numpy().flatten()\n",
        "\n",
        "mse_gd = mean_squared_error(y_test, y_pred_gd)\n",
        "r2_gd = r2_score(y_test, y_pred_gd)\n",
        "\n",
        "print(f\"Gradient Descent — MSE: {mse_gd:.4f}, R²: {r2_gd:.4f}\")\n",
        "print(f\"scikit-learn    — MSE: {mse_multi:.4f}, R²: {r2_multi:.4f}\")\n",
        "print(f\"NumPy (OLS)     — MSE: {mse_numpy:.4f}, R²: {r2_numpy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1QKXgVIMc9O"
      },
      "source": [
        "Gradient descent gets very close to the exact solution, but may not match it perfectly — it's an *approximation* that gets better with more epochs and careful tuning of the learning rate. For linear regression, OLS is better. But gradient descent scales to problems where OLS can't go: millions of parameters, nonlinear models, and neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0uUs-hyMc9O"
      },
      "source": [
        "## EXERCISE 3: Experiment with Gradient Descent\n",
        "\n",
        "1. Copy the training loop from above and try `learning_rate = 0.1`. What happens? Then try `learning_rate = 0.001`. How does the loss curve change?\n",
        "2. Try running for only 100 epochs vs 5000 epochs. How does the final MSE compare to scikit-learn?\n",
        "3. (Bonus) Try removing the feature scaling step (use `X_train.values` instead of `X_train_scaled`). What goes wrong?\n",
        "\n",
        "These experiments will give you intuition for how learning rate and scaling affect training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ule1fG5oMc9O"
      },
      "outputs": [],
      "source": [
        "# 1. Compare learning rates\n",
        "\n",
        "# Write your code here.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Click to reveal solution.\n",
        "\n",
        "# 1. Compare learning rates\n",
        "\n",
        "_w, _b, loss_slow = run_gd(0.001, 1000, X_t, y_t, n_features)\n",
        "_w, _b, loss_medium = run_gd(0.01, 1000, X_t, y_t, n_features)\n",
        "_w, _b, loss_fast = run_gd(0.1, 1000, X_t, y_t, n_features)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(loss_slow, label='lr=0.001 (too slow)')\n",
        "plt.plot(loss_medium, label='lr=0.01')\n",
        "plt.plot(loss_fast, label='lr=0.1 (faster)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('Effect of Learning Rate')\n",
        "plt.yscale('log')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final loss — lr=0.001: {loss_slow[-1]:.4f}\")\n",
        "print(f\"Final loss — lr=0.01:  {loss_medium[-1]:.4f}\")\n",
        "print(f\"Final loss — lr=0.1:   {loss_fast[-1]:.4f}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pyoF7VarkBBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Compare different epoch counts\n",
        "\n",
        "# Write your code here."
      ],
      "metadata": {
        "id": "KhD8Y6hwmX6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Click to reveal solution.\n",
        "\n",
        "# 2. Compare different epoch counts\n",
        "for epochs in [100, 1000, 5000]:\n",
        "    _w, _b, history = run_gd(0.01, epochs, X_t, y_t, n_features)\n",
        "    print(f\"{epochs:>5d} epochs — Final MSE: {history[-1]:.4f}\")\n",
        "\n",
        "print(f\"\\nsklearn MSE (exact):   {mse_multi:.4f}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "p6S0HPMzkJSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. (Bonus) Remove feature scaling"
      ],
      "metadata": {
        "id": "I5OwPUNQmgRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Click to reveal solution.\n",
        "\n",
        "# 3. (Bonus) Without feature scaling — loss explodes!\n",
        "X_t_unscaled = torch.tensor(X_train.values, dtype=torch.float32)\n",
        "\n",
        "try:\n",
        "    _w, _b, loss_unscaled = run_gd(0.01, 100, X_t_unscaled, y_t, n_features)\n",
        "    print(f\"Final loss (unscaled): {loss_unscaled[-1]:.4f}\")\n",
        "    if np.isnan(loss_unscaled[-1]) or np.isinf(loss_unscaled[-1]):\n",
        "        print(\"Loss exploded to NaN/Inf! Scaling is essential for gradient descent.\")\n",
        "except:\n",
        "    print(\"Training failed — gradients exploded without scaling.\")\n",
        "\n",
        "# Even a tiny learning rate barely works\n",
        "_w, _b, loss_unscaled_tiny = run_gd(0.0000001, 1000, X_t_unscaled, y_t, n_features)\n",
        "print(f\"\\nWith lr=1e-7 (unscaled), final loss after 1000 epochs: {loss_unscaled_tiny[-1]:.4f}\")\n",
        "print(f\"With lr=0.01 (scaled),   final loss after 1000 epochs:   {loss_medium[-1]:.4f}\")\n",
        "print(\"\\nWithout scaling, you need an extremely small learning rate and many more epochs.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "x76Av3ASmaIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFc3ytfSMc9O"
      },
      "source": [
        "---\n",
        "# Section 4: Putting It All Together\n",
        "\n",
        "Let's compare all three approaches side by side."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuLcPMElMc9O"
      },
      "outputs": [],
      "source": [
        "results = pd.DataFrame({\n",
        "    'Method': ['sklearn (1 feature)', 'sklearn (all features)', 'NumPy OLS', 'PyTorch GD'],\n",
        "    'MSE': [mse_simple, mse_multi, mse_numpy, mse_gd],\n",
        "    'R²': [r2_simple, r2_multi, r2_numpy, r2_gd]\n",
        "})\n",
        "\n",
        "print(results.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKhKAtQFMc9O"
      },
      "outputs": [],
      "source": [
        "# Visual comparison: predicted vs actual for all three multi-feature models\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "models_data = [\n",
        "    ('scikit-learn', y_pred_multi, r2_multi),\n",
        "    ('NumPy OLS', y_pred_numpy, r2_numpy),\n",
        "    ('PyTorch GD', y_pred_gd, r2_gd)\n",
        "]\n",
        "\n",
        "for ax, (name, preds, r2) in zip(axes, models_data):\n",
        "    ax.scatter(y_test, preds, alpha=0.15, s=8)\n",
        "    ax.plot([0, 5], [0, 5], 'r--', linewidth=2)\n",
        "    ax.set_xlabel('Actual')\n",
        "    ax.set_ylabel('Predicted')\n",
        "    ax.set_title(f'{name} (R² = {r2:.4f})')\n",
        "    ax.set_xlim(0, 5.2)\n",
        "    ax.set_ylim(0, 5.2)\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxqbl5BBMc9P"
      },
      "source": [
        "## EXERCISE 4: Baseline Comparison\n",
        "\n",
        "Remember from the lecture: the simplest \"model\" is to always predict the mean of the training target.\n",
        "\n",
        "1. Compute the MSE and R² of a mean-only baseline (predict `y_train.mean()` for every test sample).\n",
        "2. How much better is the full linear regression model compared to this baseline?\n",
        "3. What R² value does the mean baseline give? Does that make sense mathematically?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKA7KTWUMc9P"
      },
      "outputs": [],
      "source": [
        "# Compute the mean baseline MSE and R²\n",
        "\n",
        "# Write your code here."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Click to reveal solution.\n",
        "\n",
        "# Mean baseline: predict y_train.mean() for every test sample\n",
        "y_pred_baseline = np.full(len(y_test), y_train.mean())\n",
        "\n",
        "mse_baseline = mean_squared_error(y_test, y_pred_baseline)\n",
        "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
        "\n",
        "print(f\"Mean baseline — MSE: {mse_baseline:.4f}, R²: {r2_baseline:.4f}\")\n",
        "print(f\"Linear reg    — MSE: {mse_multi:.4f}, R²: {r2_multi:.4f}\")\n",
        "print(f\"\\nLinear regression reduces MSE by {(1 - mse_multi / mse_baseline) * 100:.1f}% over the baseline.\")\n",
        "print(f\"\\nR² of the mean baseline is ~0 by definition — it explains none of the variance.\")\n",
        "print(f\"(The slight deviation from exactly 0 is because the test set mean differs slightly from the training set mean.)\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OT19trGlnSKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsAr_qj1Mc9P"
      },
      "source": [
        "---\n",
        "# Summary\n",
        "\n",
        "You've now implemented linear regression three different ways:\n",
        "\n",
        "- **scikit-learn**: The practical choice. Clean API, handles details for you. Use this in your projects.\n",
        "- **NumPy (Normal Equation)**: Shows you the exact math. Gives the same answer as sklearn. Useful for understanding what's happening under the hood.\n",
        "- **PyTorch (Gradient Descent)**: An iterative approximation. Slightly less precise for linear regression, but this exact approach scales to neural networks with millions of parameters.\n",
        "\n",
        "Key takeaways:\n",
        "- All three methods are solving the **same optimization problem**: minimize MSE\n",
        "- More features generally help, but the improvement isn't always dramatic\n",
        "- Gradient descent requires **feature scaling** and **hyperparameter tuning** (learning rate, epochs)\n",
        "- Always compare against a simple **baseline** (like predicting the mean) to know if your model is actually useful"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}